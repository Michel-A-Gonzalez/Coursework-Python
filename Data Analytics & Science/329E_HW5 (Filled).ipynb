{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "CS329e_HW5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfuIVwk6VQNg"
      },
      "source": [
        "## C S 329E HW 5\n",
        "\n",
        "# Crossvalidation and hyperparameter selection\n",
        "\n",
        "## Michel Gonzalez, Group: 11\n",
        "\n",
        "For this week's homework we are going to explore the cross validation testing methodology and applying that to get error estimates on the two algorithms we've used so far:\n",
        "  - Linear Regression\n",
        "  - Decision Tree classification\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96E_96hMVRiM"
      },
      "source": [
        "# Google colab's default version of scikit-learn isn't the latest, so you will \n",
        "# need to update the virtual machine and restart the runtime\n",
        "#!pip install scikit-learn==1.0.2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDHKJDFmVQNi"
      },
      "source": [
        "# import the libraries! If you want to add things here for visualization, please do, \n",
        "# but only use sklearn when prompted\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import tree \n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.datasets import load_diabetes"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZs2nuNbVQNj"
      },
      "source": [
        "# Part 1 - Calculate Generalized Error on Linear Regression with k-fold Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JVk1iaWVQNk"
      },
      "source": [
        "## Q1.1 Load in the diabetes data set as a pandas dataframe and series.  \n",
        "Documentation on the data set is [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html).  Name your features dataframe (the independent variables) `df_X` and your target value (the dependent variable) series `s_y`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQVbOpO9VQNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72eb0ac2-557d-44e8-be18-46892d9654eb"
      },
      "source": [
        "df_X, s_y_ordered = load_diabetes(return_X_y=True, as_frame=True)\n",
        "\n",
        "# adds the intercept column for later use in beta_hat, and also shuffles the data \n",
        "# for better traning of our model\n",
        "df_X = pd.concat([pd.DataFrame({'Intercept' : np.ones(len(df_X))}), df_X], axis = 1)\n",
        "\n",
        "# Shuffles our data and reorders the \n",
        "# target data by using the new index order\n",
        "df_X = df_X.sample(frac = 1)\n",
        "\n",
        "new_idx = df_X.index\n",
        "\n",
        "s_y = s_y_ordered.copy(deep = True)\n",
        "\n",
        "for i in range(0, len(s_y)):\n",
        "\n",
        "  s_y[i] = s_y_ordered[new_idx[i]]\n",
        "\n",
        "len(df_X), len(s_y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(442, 442)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzgW4G5SVQNk"
      },
      "source": [
        "## Q1.2 Define a function that creates a linear least squares regression model and a function to predict a continuous value given a linear regression model\n",
        "The first function should take in two parameters, `df_X`, and `s_y` and return the least squares regression model, $\\hat{\\beta}$ (using the notation from the ESLII book chapter 3 and HW3).  You can not use any libraries outside of pandas and numpy. Note that the length of beta_hat should be the number of columns in `df_X` + 1. \n",
        "\n",
        "The second function should take in two parameters, `beta_hat` - the model generated from the `get_linear_regression` function, and `df_X` - that has the attribute values at which we want to predict a continuous value.  We assume that the format and ordering of `df_X` used to create the model and `df_X` used to generate predictions are consistent. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf37fNLUVQNl"
      },
      "source": [
        "def get_linear_regression_model( df_X, s_y ):\n",
        "    # your code here\n",
        "\n",
        "    #beta_hat = (X * XT)^-1 * XT * y\n",
        "    # uses linear lstsquares method to solve for beta_hat\n",
        "    beta_hat, residuals, rank, s = np.linalg.lstsq(df_X, s_y, rcond = None )\n",
        "\n",
        "    return pd.Series(beta_hat)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaM8sfLCVQNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b460c4-f50b-4573-a380-66b375e2833e"
      },
      "source": [
        "# code to check beta_hat\n",
        "np.random.seed(23)\n",
        "beta_hat = get_linear_regression_model( pd.DataFrame(np.random.random((34,4))), pd.Series(np.random.random(34)*10.0) )\n",
        "print(beta_hat)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    3.682765\n",
            "1    3.484999\n",
            "2    0.001342\n",
            "3    2.192263\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPpM4U6YVQNm"
      },
      "source": [
        "def predict_linear_regression_value( beta_hat, df_X ):\n",
        "    # your code here\n",
        "\n",
        "    # Multiplies the data set by the beta_hat that was created\n",
        "    # and creates a series s_y_hat with predicted values\n",
        "    s_y_hat = np.matmul(df_X, beta_hat)\n",
        "\n",
        "    return np.array(s_y_hat)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyMGfxtxXNCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c33113e4-99eb-40ca-fcf5-ca67c36e842f"
      },
      "source": [
        "predicted_vals = predict_linear_regression_value( beta_hat, pd.DataFrame(np.random.random((3,4))))\n",
        "predicted_vals"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.27838225, 5.10744585, 2.45066005])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRxs-OTiVQNn"
      },
      "source": [
        "## Q1.3 Define a function that partitions the dataframe and series data into dictionaries\n",
        "This function should take in three parameters, `df_X`, `s_y`, and `k`, and returns a tuple of two dictionaries.\n",
        "In both dictionaries the key is the `k` value (an integer from one to `k` inclusive).\n",
        "The first dictionary will have the dataframe of the training data that contains approximately $\\frac{1}{k}$ of the data (variation due to randomness is acceptable).\n",
        "The second dictionary will have the series of the target data that contains approximately $\\frac{1}{k}$ of the data (variation due to randomness is acceptable). Please note the targets _must match_ the same rows as the dataframe at this index, e.g, the length of the kth partition is the same for the dataframe and series.\n",
        "\n",
        "Call that function with $k=5$ and create the dictionaries `dict_k_df_X` and `dict_k_s_y`. Print out the number of rows in each fold.  Check that the number of data points in each partition totals the number of data points in the entire dataset. \n",
        "\n",
        "Here is some example output from checking the length of the folds:\n",
        "```\n",
        "Fold 1 length of dataframe is 88 and length of series is 88\n",
        "Fold 2 length of dataframe is 96 and length of series is 96\n",
        "Fold 3 length of dataframe is 88 and length of series is 88\n",
        "Fold 4 length of dataframe is 79 and length of series is 79\n",
        "Fold 5 length of dataframe is 91 and length of series is 91\n",
        "The sum of the number of elements in each fold is 442 and there are 442 rows in the original df\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKZFErNNVQNo"
      },
      "source": [
        "def partition_data( df_X, s_y, k):\n",
        "    # your code here\n",
        "\n",
        "    # Creates the dictionaries that will hold each fold and series of data\n",
        "    # it is assumed that df_X was properly shuffled before partitioning\n",
        "    dict_k_df_X = {}\n",
        "\n",
        "    dict_k_s_y = {}\n",
        "\n",
        "    # partitions the data set objects by using a random number selcted from 1 through k inclusive\n",
        "    # then it creates each dictionary\n",
        "    random_indx = np.random.randint(1, k + 1, size = len(s_y))\n",
        "\n",
        "    for i in range (1, k + 1):\n",
        "\n",
        "        dict_k_df_X[i] = df_X[random_indx == i]\n",
        "\n",
        "        dict_k_s_y[i] = np.array(s_y[random_indx == i])\n",
        "\n",
        "    return dict_k_df_X, dict_k_s_y"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pefH5wfJVQNo"
      },
      "source": [
        "(dict_k_df_X, dict_k_s_y) = partition_data( df_X, s_y, 5 )"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ27tMRFVQNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "509df294-ee95-427a-ed4e-273ac35946e6"
      },
      "source": [
        "# Check fold sizes\n",
        "\n",
        "total = 0\n",
        "\n",
        "for i in range (1, 6):\n",
        "  \n",
        "  print('Fold ' + str(i) + ' length of dataframe is ' + str(len(dict_k_df_X[i])) + ' and length of series is ' + str(len(dict_k_s_y[i])))\n",
        "\n",
        "  total += len(dict_k_df_X[i])\n",
        "\n",
        "print('The sum of the number of elements in each fold is ' + str(total) + ' and there are ' + str(len(df_X)) + ' rows in the original df')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 length of dataframe is 91 and length of series is 91\n",
            "Fold 2 length of dataframe is 98 and length of series is 98\n",
            "Fold 3 length of dataframe is 77 and length of series is 77\n",
            "Fold 4 length of dataframe is 75 and length of series is 75\n",
            "Fold 5 length of dataframe is 101 and length of series is 101\n",
            "The sum of the number of elements in each fold is 442 and there are 442 rows in the original df\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtdijUdNVQNp"
      },
      "source": [
        "## Q1.4 Define a function that calculates a regression metric\n",
        "This function should accept two series of equal length $n$ numpy arrays, `s_y`, and `s_y_hat`. The metric it should calculate is the mean absolute error, $MAE = \\sum\\limits_{i=1}^n\\frac{|{s\\_y_i - {s\\_y\\_hat}_i}|}{n}$ \n",
        "\n",
        "Test your function by using the vectors:\n",
        "```\n",
        "x = np.array([1,2,3])\n",
        "y = np.array([2,2,3])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v40R9HM_VQNq"
      },
      "source": [
        "def get_mae(s_y, s_y_hat):\n",
        "    # your code here\n",
        "\n",
        "    # mae = sum((s_y[i] - s_y_hat[i])/n)\n",
        "    mae = 0\n",
        "\n",
        "    # This will get the mean absolute error by using the actual values and\n",
        "    # the predicted values\n",
        "    n = len(s_y)\n",
        "\n",
        "    for i in range(0, n):\n",
        "\n",
        "      mae += abs((s_y[i] - s_y_hat[i]))/n\n",
        "\n",
        "    return float(mae)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJB-KKKvVQNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf113ddb-7740-41df-f0f8-4409e5642ccb"
      },
      "source": [
        "# Test it \n",
        "x = np.array([1,2,3])\n",
        "y = np.array([2,2,3])\n",
        "get_mae(x,y)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pgVtzgWVQNq"
      },
      "source": [
        "## Q1.5 Calculate the $MAE$ for each fold\n",
        "For each fold in your dictionaries, calculate the $MAE$.  Use the partition number key as the test set, and all other partitions as the train set. \n",
        "\n",
        "Print the min, max, and mean $MAE$ of your 5 folds. \n",
        "\n",
        "You must use your helper functions that you wrote above, `get_linear_regression_model`, `predict_linear_regression_value` and `get_mae`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuxFj8z5VQNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b70d06-122f-4823-b519-0fc23e175085"
      },
      "source": [
        "# Creates an array that will hold the MAE vlaues \n",
        "mae = np.array([])\n",
        "\n",
        "# This will access all the information in each fold [1, k]\n",
        "for k in dict_k_df_X.keys():\n",
        "    \n",
        "    # your code here\n",
        "\n",
        "    # Extracts the test data that will be used at the \n",
        "    # end of the regression\n",
        "    test_X_k = dict_k_df_X[k]\n",
        "\n",
        "    test_s_y_k = dict_k_s_y[k]\n",
        "\n",
        "    # Creates empty shells that will hold the training data\n",
        "    training_X_k = pd.DataFrame(columns = df_X.columns)\n",
        "\n",
        "    training_s_y_k = np.array([])\n",
        "\n",
        "    # compiles all the data in the other folds into one so it can be used to \n",
        "    # train the model\n",
        "    for i in dict_k_df_X.keys():\n",
        "\n",
        "      if(i != k):\n",
        "        \n",
        "        training_X_k = training_X_k.append(dict_k_df_X[i])\n",
        "\n",
        "        training_s_y_k = np.append(np.array(training_s_y_k), np.array(dict_k_s_y[i]))\n",
        "\n",
        "      else:\n",
        "\n",
        "        continue\n",
        "    \n",
        "    # Creates the beta vetcor to find for predicttions is y_hat = df_X * beta_hat\n",
        "    beta_hat_k = get_linear_regression_model(training_X_k, training_s_y_k)\n",
        "\n",
        "    # Creates the predicted values vetcor\n",
        "    s_y_hat_k = predict_linear_regression_value(beta_hat_k, test_X_k)\n",
        "\n",
        "    # Finds the mean absolute error by using the expected values vs the actule values\n",
        "    # for each fold\n",
        "    mae_val = get_mae(test_s_y_k, s_y_hat_k)\n",
        "\n",
        "    mae = np.append(mae, mae_val)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: Calling a ufunc on non-aligned DataFrames (or DataFrame/Series combination). Currently, the indices are ignored and the result takes the index/columns of the first DataFrame. In the future , the DataFrames/Series will be aligned before applying the ufunc.\n",
            "Convert one of the arguments to a NumPy array (eg 'ufunc(df1, np.asarray(df2)') to keep the current behaviour, or align manually (eg 'df1, df2 = df1.align(df2)') before passing to the ufunc to obtain the future behaviour and silence this warning.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OxejniBVQNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3742792c-18e0-4743-dc9d-37250d35daa8"
      },
      "source": [
        "print(\"The min MAE is {:.2f}, the max MAE is {:.2f}, and the mean MAE is {:.2f}\".format(mae.min(),mae.max(),mae.mean()))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The min MAE is 42.54, the max MAE is 45.79, and the mean MAE is 44.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqOqK30FVQNr"
      },
      "source": [
        "# Part 2 - Find the best hyperparameter to use in a Decision Tree "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xVkJfkdVQNr"
      },
      "source": [
        "## Q2.1 Load the iris data in as a pandas dataframe and a series\n",
        "Documentation on the data set is [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html). Name your features dataframe (the independent variables) `df_X` and your class label (the dependent variable) series `s_y`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oBRRztuVQNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f95cc40-5257-4b83-93b6-9330f6f0e548"
      },
      "source": [
        "df_X, s_y_ordered = load_iris(return_X_y=True, as_frame=True)\n",
        "\n",
        "# This will shuffle the data and reorder the target data\n",
        "# by the new index\n",
        "df_X = df_X.sample(frac = 1)\n",
        "\n",
        "new_idx = df_X.index\n",
        "\n",
        "s_y = s_y_ordered.copy(deep = True)\n",
        "\n",
        "for i in range(0, len(s_y)):\n",
        "\n",
        "  s_y[i] = s_y_ordered[new_idx[i]]\n",
        "\n",
        "len(df_X), len(s_y)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 150)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0OUEFGaVQNr"
      },
      "source": [
        "## Q2.2 Partition `df_X` and `s_y` into $5$ partitions of roughly equal size\n",
        "Make 2 dictionaries, with the key of each dictionary the fold number.  The value of the dictionary `dict_k_df_X` is the $k^{th}$ partition of the data, and the value of the dictionary `dict_k_s_y` is the corresponding $k^{th}$ target classification.  Print out the number of rows in each fold.  Check that the number of data points in each partition totals the number of data points in the entire dataset. \n",
        "\n",
        "Note, you can reuse the functions from Section 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FDi6t03VQNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a93f3bb-75d6-4566-90f3-7a9d80f82f2d"
      },
      "source": [
        "# this will partiition our data into similar but still random lengths\n",
        "(dict_k_df_X, dict_k_s_y) = partition_data( df_X, s_y, 5 )\n",
        "\n",
        "total = 0\n",
        "\n",
        "for i in range (1, 6):\n",
        "  \n",
        "  print('Fold ' + str(i) + ' length of dataframe is ' + str(len(dict_k_df_X[i])) + ' and length of series is ' + str(len(dict_k_s_y[i])))\n",
        "\n",
        "  total += len(dict_k_df_X[i])\n",
        "\n",
        "print('The sum of the number of elements in each fold is ' + str(total) + ' and there are ' + str(len(df_X)) + ' rows in the original df')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 length of dataframe is 23 and length of series is 23\n",
            "Fold 2 length of dataframe is 37 and length of series is 37\n",
            "Fold 3 length of dataframe is 29 and length of series is 29\n",
            "Fold 4 length of dataframe is 28 and length of series is 28\n",
            "Fold 5 length of dataframe is 33 and length of series is 33\n",
            "The sum of the number of elements in each fold is 150 and there are 150 rows in the original df\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3QvbgqsVQNs"
      },
      "source": [
        "## Q2.3 Define a function that calculates accuracy\n",
        "The function should accept two series and compare each element for equality.  The accuracy is the number of equal elements divided by the total number of elements.\n",
        "\n",
        "Test your accuracy function by calling it with the `s_y` loaded from the iris data set and an array of the same length containing all $1$ values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGEZaKCRVQNs"
      },
      "source": [
        "def get_acc( s_1, s_2 ):\n",
        "    # your code here\n",
        "\n",
        "    # Gets the accuracy by using the boolean array outpu and \n",
        "    # counting all the true vales, then divide by the total\n",
        "    # number of elements \n",
        "    n = len(s_1)\n",
        "\n",
        "    acc_array = np.array(s_1 == s_2)\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for i in range(0, len(acc_array)):\n",
        "\n",
        "      if(acc_array[i] == True):\n",
        "\n",
        "        count += 1\n",
        "\n",
        "      else:\n",
        "\n",
        "        continue\n",
        "    \n",
        "    acc = count/n\n",
        "\n",
        "    return acc"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8WcPX7IVQNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "515cb2b1-d3e1-4881-a6c5-287ad32859d5"
      },
      "source": [
        "get_acc(s_y,np.ones(len(s_y)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1szoVAVkVQNs"
      },
      "source": [
        "## Q2.4 Using Nested Cross validation, find the best hyperparameter\n",
        "Use the [Decision Tree Classifier](https://scikit-learn.org/stable/modules/tree.html#classification) class to build a decision tree inside of a 5-fold cross validation loop.  The partitions you already created in 2.2 will be the outer loop.  In the inside loop you should use 4-fold cross validation (so you don't have to partition _again_) to find the best value for `min_impurity_decrease`.  Use the Gini Index as your impurity measure. \n",
        "    Calculate the mean accuracy across the 4 folds of your inner loop for all the candidate `min_impurity_decrease` values, and print the value.  Use the array `np.array([0.1,0.25,0.3,0.4])` as the candidates for the best hyperparameter. If there is a tie (two `min_impurity_decrease` values give the same highest accuracy), choose the lowest `min_impurity_decrease` value. \n",
        "\n",
        "For each inner loop, select the best `min_impurity_decrease` and train the outer fold training data on using that value. \n",
        "\n",
        "For each of the 5 executions of the inner loop, your output should look something like this:\n",
        "```\n",
        "Testing 0.10 min impurity decrease\n",
        "\tAverage accuracy over 4 folds is 0.95\n",
        "Testing 0.25 min impurity decrease\n",
        "\tAverage accuracy over 4 folds is 0.86\n",
        "Testing 0.30 min impurity decrease\n",
        "\tAverage accuracy over 4 folds is 0.63\n",
        "Testing 0.40 min impurity decrease\n",
        "\tAverage accuracy over 4 folds is 0.27\n",
        "\n",
        "Best min impurity decrease is 0.1\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a7spNAnVQNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8add55cc-55c6-42ba-a63d-f34c20039209"
      },
      "source": [
        "possible_min_impurity_decrease = np.array([0.1,0.25,0.3,0.4])\n",
        "\n",
        "# Outer loop\n",
        "outer_acc = np.array([])\n",
        "\n",
        "# This will get the number of inner loop\n",
        "# folds to get average\n",
        "div = max(dict_k_df_X.keys()) - 1\n",
        "\n",
        "for k in dict_k_df_X.keys():\n",
        "  # your code here\n",
        "\n",
        "  # Creates the test and empty traning sets\n",
        "  test_k_df_X = dict_k_df_X[k]\n",
        "\n",
        "  test_k_s_y = dict_k_s_y[k]\n",
        "\n",
        "  training_k_df_X = pd.DataFrame(columns = df_X.columns)\n",
        "\n",
        "  training_k_s_y = np.array([])\n",
        "\n",
        "  # This is used to keep track of the accuracy\n",
        "  # from each min impurity decrease\n",
        "  acc = []\n",
        "\n",
        "  for pos_min_impurity in possible_min_impurity_decrease:\n",
        "\n",
        "    clf = tree.DecisionTreeClassifier(criterion = 'gini', min_impurity_decrease = pos_min_impurity)\n",
        "\n",
        "    # Inner loop cross validation code here (use 4 folds, where the fold does not include k)\n",
        "\n",
        "    avg = 0\n",
        "\n",
        "    # Cycles through the 4 folds using one as the test for the modle\n",
        "    # and the other 3 folds as the traning sets\n",
        "    for j in dict_k_df_X.keys():\n",
        "\n",
        "      if(j != k):\n",
        "\n",
        "        test_j_df_X = dict_k_df_X[j]\n",
        "\n",
        "        test_j_s_y = dict_k_s_y[j]\n",
        "\n",
        "        training_j_df_X = pd.DataFrame(columns = df_X.columns)\n",
        "\n",
        "        training_j_s_y = np.array([])\n",
        "\n",
        "        # creates the inner traning sets for each test fold\n",
        "        for i in dict_k_df_X.keys():\n",
        "\n",
        "          if(i != k and i != j):\n",
        "\n",
        "            training_j_df_X = training_j_df_X.append(dict_k_df_X[j])\n",
        "\n",
        "            training_j_s_y = np.append(np.array(training_j_s_y), np.array(dict_k_s_y[j]))\n",
        "\n",
        "          else:\n",
        "\n",
        "            continue\n",
        "\n",
        "        # Trains a tree and gets its accuracy    \n",
        "        clf = clf.fit(training_j_df_X, training_j_s_y)\n",
        "\n",
        "        s_y_hat_j = clf.predict(test_j_df_X)\n",
        "\n",
        "        accuracy = get_acc(s_y_hat_j, test_j_s_y)\n",
        "\n",
        "        avg += accuracy\n",
        "\n",
        "\n",
        "      else:\n",
        "\n",
        "        continue\n",
        "\n",
        "    avg = avg/div\n",
        "\n",
        "    acc.append(avg)\n",
        "\n",
        "    print('Testing', pos_min_impurity, 'min impurity decrease')\n",
        "    print('    Average accuracy over 4 folds is ' + str(format(avg, '2.4f')))\n",
        "  \n",
        "  # Use best min impurity decrease to train model\n",
        "\n",
        "  # Finds best min impurity decrese by finding the first maximum accuracy\n",
        "  # (this will pick the lower min impurity decrease in case of a tie aslong as the list of values is in accending order)\n",
        "  max_val = max(acc)\n",
        "\n",
        "  max_indx = acc.index(max_val)\n",
        "\n",
        "  print()\n",
        "  print('Best min impurity decrease is', possible_min_impurity_decrease[max_indx])\n",
        "  print()\n",
        "\n",
        "  # Makes the traning set for the outer loop folds\n",
        "  for m in dict_k_df_X.keys():\n",
        "\n",
        "    if(m == k):\n",
        "\n",
        "      continue\n",
        "    \n",
        "    else: \n",
        "\n",
        "      training_k_df_X = training_k_df_X.append(dict_k_df_X[m])\n",
        "\n",
        "      training_k_s_y = np.append(np.array(training_k_s_y), np.array(dict_k_s_y[m]))\n",
        "\n",
        "  # outer accuracy calculation\n",
        "\n",
        "  clf = tree.DecisionTreeClassifier(min_impurity_decrease = possible_min_impurity_decrease[max_indx])\n",
        "\n",
        "  clf = clf.fit(training_k_df_X, training_k_s_y)\n",
        "\n",
        "  s_y_hat_k = clf.predict(test_k_df_X)\n",
        "\n",
        "  this_acc = get_acc(s_y_hat_k, test_k_s_y)\n",
        "\n",
        "  outer_acc = np.append(outer_acc,this_acc) # make sure and calculate this_acc in your loop"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing 0.1 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.9848\n",
            "Testing 0.25 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.9470\n",
            "Testing 0.3 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.8934\n",
            "Testing 0.4 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.4038\n",
            "\n",
            "Best min impurity decrease is 0.1\n",
            "\n",
            "Testing 0.1 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.9848\n",
            "Testing 0.25 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.9470\n",
            "Testing 0.3 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.8282\n",
            "Testing 0.4 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.4137\n",
            "\n",
            "Best min impurity decrease is 0.1\n",
            "\n",
            "Testing 0.1 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.9848\n",
            "Testing 0.25 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.9470\n",
            "Testing 0.3 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.8282\n",
            "Testing 0.4 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.4154\n",
            "\n",
            "Best min impurity decrease is 0.1\n",
            "\n",
            "Testing 0.1 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.9848\n",
            "Testing 0.25 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.9470\n",
            "Testing 0.3 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.8818\n",
            "Testing 0.4 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.3855\n",
            "\n",
            "Best min impurity decrease is 0.1\n",
            "\n",
            "Testing 0.1 min impurity decrease\n",
            "    Average accuracy over 4 folds is 1.0000\n",
            "Testing 0.25 min impurity decrease\n",
            "    Average accuracy over 4 folds is 1.0000\n",
            "Testing 0.3 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.8812\n",
            "Testing 0.4 min impurity decrease\n",
            "    Average accuracy over 4 folds is 0.3879\n",
            "\n",
            "Best min impurity decrease is 0.1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6-dgURkVQNt"
      },
      "source": [
        "## Q2.5 Show the generalized performance of the classifier \n",
        "Show the generalized performance of the classifier by printing the min, max, and mean accuracy of the outer fold test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wed-YLSVQNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b1820e-bd4f-4de7-a4bc-b309a33e0649"
      },
      "source": [
        "print(\"The min accuracy is {:.2f}, the max accuracy is {:.2f}, and the mean accuracy is {:.2f}\".format(outer_acc.min(),outer_acc.max(),outer_acc.mean()))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The min accuracy is 0.85, the max accuracy is 0.97, and the mean accuracy is 0.93\n"
          ]
        }
      ]
    }
  ]
}