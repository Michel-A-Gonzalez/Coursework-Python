{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS329E_HW9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXmCHcwKs6rd"
      },
      "source": [
        "## C S 329E HW8\n",
        "\n",
        "# Image classification using the FMNIST dataset and PyTorch\n",
        "\n",
        "## Michel Gonzalez - Mag9989\n",
        "\n",
        "This is the companion notebook that you will hand in to show that you \n",
        "were following along the LinkedIn Learning Video.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1 - Working with FMNIST\n",
        "\n",
        "Add in the code from the first exercise file that trained a neural network and tested the network on test data.  Use index 23 to test the network.  Was it correct? If not, what class do you think it should have predicted?"
      ],
      "metadata": {
        "id": "IlppPnzH5N2I"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzCCniVwNTdp"
      },
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCJzXv0OK1Bs"
      },
      "source": [
        "from torch._C import TracingState\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "mean, std = (0.5,), (0.5,)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean, std)\n",
        "                                ])\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download =True, train = True, transform = transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
        "\n",
        "testset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download =True, train = True, transform = transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqMqFbIVrbFH"
      },
      "source": [
        "class FMNIST (nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(784, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "  def forward (self, x):\n",
        "\n",
        "    x = x.view(x.shape[0], -1)\n",
        "\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "model = FMNIST()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNNyI5YRZ7H1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "294d5a4d-5678-4451-d060-7cd10879dff9"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for i in range(num_epochs):\n",
        "\n",
        "  cum_loss = 0\n",
        "\n",
        "  for images, labels in trainloader:\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(images)\n",
        "\n",
        "    loss = criterion(output, labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    cum_loss += loss.item()\n",
        "\n",
        "  print(f\"Traning loss: {cum_loss/len(trainloader)}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traning loss: 1.0341349975513752\n",
            "Traning loss: 0.5616475922593684\n",
            "Traning loss: 0.49201103581040145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWYw7ZOzsS8U"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "images, labels = next(iter(testloader))\n",
        "\n",
        "test_image_id = 23\n",
        "\n",
        "img = images[test_image_id].view(1,784)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  logps = model(img)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBf23XrtqrB6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "08a9ec96-8092-4623-976f-484cbd6525eb"
      },
      "source": [
        "ps = torch.exp(logps)\n",
        "nps = ps.numpy()[0]\n",
        "FMNIST_labels = ['T-shirt/top', 'Trouser', 'Pullover', ' Dress', 'Coat', 'Sandal', 'Shirt', 'Sport Shoes', 'Bag', 'Ankle Boot']\n",
        "plt.xticks(np.arange(10), labels = FMNIST_labels, rotation = 'vertical')\n",
        "plt.bar(np.arange(10), nps)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 10 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7gY5hARpOp4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "63b0925b-098f-4284-fc75-7c4c9d2a4a22"
      },
      "source": [
        "def denormalize(tensor):\n",
        "  tensor = tensor * 0.5 + 0.5\n",
        "  return tensor\n",
        "\n",
        "img = img.view(28,-1)\n",
        "img = denormalize(img)\n",
        "plt.imshow(img, cmap = 'gray')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0da29f1c90>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think that the classifier was correct for this image"
      ],
      "metadata": {
        "id": "ciQDhmsjxKJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2 Tensors\n",
        "\n",
        "Add in the code from working with Tensors.  Type along with the instructor."
      ],
      "metadata": {
        "id": "jNpnsW_c7POG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "9cRCGG7P7P3t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.zeros(4, 3)\n",
        "w"
      ],
      "metadata": {
        "id": "okVPDwtc8R34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "522304b1-dddb-42da-d081-af01c49c3810"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w.size()"
      ],
      "metadata": {
        "id": "zjDN18aE8aP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b70d2244-0ef3-47ca-9516-34447cad9822"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w.shape"
      ],
      "metadata": {
        "id": "QVt1igOx8iEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df9ba85-13a7-4464-fd38-5b24fc14686b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn(3,4)\n",
        "w"
      ],
      "metadata": {
        "id": "34gY69y98jYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5f6d693-2c2e-446b-ebb5-eec99ee06270"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.0489,  0.7899, -1.8661, -0.7692],\n",
              "        [-0.1590, -1.6144, -0.9722, -1.6303],\n",
              "        [ 1.3687,  0.3133,  0.2125,  0.5706]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.randn_like(w)\n",
        "t"
      ],
      "metadata": {
        "id": "ub9ep2US8uwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "823088ce-b1fe-46fd-8a91-4009e3762d27"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5458, -0.5508,  0.4456,  0.0403],\n",
              "        [-1.3119, -0.6464,  0.8553, -0.6457],\n",
              "        [ 0.6277,  0.2145, -0.5673,  0.3169]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w.fill_(1)"
      ],
      "metadata": {
        "id": "UyH1PofA88Rb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb72360-6ae5-466a-a501-533369d88952"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = w.view(3,4)\n",
        "t"
      ],
      "metadata": {
        "id": "LGXR8rWe9RD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41ab3491-7721-45ac-a8bc-82f1022ec82d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w.view(3, -1)"
      ],
      "metadata": {
        "id": "qa40d5559mcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87490d62-f1ab-47b3-ba11-88f7060f1057"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(w.numpy())"
      ],
      "metadata": {
        "id": "cQjCtOo194OS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d24af00-ed54-47e5-990f-2dece12c004c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3 - Autograd\n",
        "\n",
        "Copy over the code from the Autograd notebook, and execute the cells and enter the code with the instructor."
      ],
      "metadata": {
        "id": "DTk-e8DN-cIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "dzXMtYVJ-ck0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import TracingState\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "mean, std = (0.5,), (0.5,)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean, std)\n",
        "                                ])\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download =True, train = True, transform = transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
        "\n",
        "testset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download =True, train = True, transform = transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True)"
      ],
      "metadata": {
        "id": "ul6IakOBC-2c"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FMNIST (nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(784, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "  def forward (self, x):\n",
        "\n",
        "    x = x.view(x.shape[0], -1)\n",
        "\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "#model = FMNIST()"
      ],
      "metadata": {
        "id": "xLlorh4gC_Cc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128,64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64,10),\n",
        "                      nn.LogSoftmax(dim=1))"
      ],
      "metadata": {
        "id": "64eANIpzC_Iv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ME_m0xp3fxs",
        "outputId": "d420c0ac-bd8b-4e93-bb89-18857703b5a5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=784, out_features=128, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(trainloader))\n",
        "\n",
        "images = images.view(images.shape[0], -1)"
      ],
      "metadata": {
        "id": "8CHOHbjgC_NP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "LslendNbC_Q2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Initial weigts gradient : ', model[0].weight.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KEoGyNv3j1k",
        "outputId": "f5d4b966-14ea-4337-d61f-3c704465cb84"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weigts gradient :  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(images)\n",
        "loss = criterion(output, labels)\n",
        "loss.backward()\n",
        "print('Initial weights : ', model[0].weight)\n",
        "print('Initial weigts gradient : ', model[0].weight.grad)"
      ],
      "metadata": {
        "id": "RgQ6eHsSDQPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6704263-d2b8-4e60-ebdb-dcc9861fb9d8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
            "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0295,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0220, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0286,  0.0350, -0.0105]],\n",
            "       requires_grad=True)\n",
            "Initial weigts gradient :  tensor([[-0.0030, -0.0030, -0.0030,  ..., -0.0030, -0.0030, -0.0030],\n",
            "        [ 0.0022,  0.0022,  0.0022,  ...,  0.0024,  0.0022,  0.0022],\n",
            "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n",
            "        ...,\n",
            "        [ 0.0014,  0.0014,  0.0014,  ...,  0.0014,  0.0014,  0.0014],\n",
            "        [ 0.0021,  0.0021,  0.0021,  ...,  0.0022,  0.0021,  0.0021],\n",
            "        [ 0.0038,  0.0038,  0.0038,  ...,  0.0038,  0.0038,  0.0038]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4 - Autograd with Tensors\n",
        "\n",
        "Type the code and execute the cells and enter the code with the instructor."
      ],
      "metadata": {
        "id": "kj8SaJG8FJdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "pxm-Np7BFD29"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w  = torch.randn(4,3,requires_grad = True)"
      ],
      "metadata": {
        "id": "ceJTOGohFutX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w"
      ],
      "metadata": {
        "id": "-zE2CcgJFuwU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29e22a17-3c24-491a-c600-d19f9bf13032"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.7536,  0.9583,  0.7206],\n",
              "        [ 0.4893, -0.9620, -0.1192],\n",
              "        [ 1.0489,  0.7899, -1.8661],\n",
              "        [-0.7692, -0.1590, -1.6144]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w.requires_grad_(False)\n",
        "w"
      ],
      "metadata": {
        "id": "Ebn91fvgFuzC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "091c9174-430b-4ef8-e125-8dfe87b36f08"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.7536,  0.9583,  0.7206],\n",
              "        [ 0.4893, -0.9620, -0.1192],\n",
              "        [ 1.0489,  0.7899, -1.8661],\n",
              "        [-0.7692, -0.1590, -1.6144]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w.requires_grad_(True)\n",
        "w"
      ],
      "metadata": {
        "id": "QNKjC290Fu11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ba19af6-6e0f-4bdb-a2ed-643d428822f0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.7536,  0.9583,  0.7206],\n",
              "        [ 0.4893, -0.9620, -0.1192],\n",
              "        [ 1.0489,  0.7899, -1.8661],\n",
              "        [-0.7692, -0.1590, -1.6144]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.exp(w)\n",
        "\n",
        "print(y)"
      ],
      "metadata": {
        "id": "L6NE903iFu4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2699df91-004b-4017-d3e0-23d0f2f06d15"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.1246, 2.6072, 2.0556],\n",
            "        [1.6312, 0.3821, 0.8876],\n",
            "        [2.8547, 2.2033, 0.1547],\n",
            "        [0.4634, 0.8530, 0.1990]], grad_fn=<ExpBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.grad_fn)"
      ],
      "metadata": {
        "id": "IEYffsspFu7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "816be972-b4a4-45a7-a61c-bb0a10561fd3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<ExpBackward0 object at 0x7f0da32acb10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outp = y.mean()\n",
        "print(outp)"
      ],
      "metadata": {
        "id": "ve3X3WsdG0uM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a8fa91-e0b3-4c2c-9900-baac2063f784"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.3680, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad)"
      ],
      "metadata": {
        "id": "hm5jpI9EG0zZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95482c28-02c8-4809-f377-2071995bd5d3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outp.backward()"
      ],
      "metadata": {
        "id": "OqCDIMtDHA2-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad)"
      ],
      "metadata": {
        "id": "6WZL4AFWHLXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5abb8d13-b744-4bef-f6c0-d41340d9098f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1770, 0.2173, 0.1713],\n",
            "        [0.1359, 0.0318, 0.0740],\n",
            "        [0.2379, 0.1836, 0.0129],\n",
            "        [0.0386, 0.0711, 0.0166]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.detach())"
      ],
      "metadata": {
        "id": "ye8dFTl5HLc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3968946b-8095-413d-8ad4-c1d5040055e9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.7536,  0.9583,  0.7206],\n",
            "        [ 0.4893, -0.9620, -0.1192],\n",
            "        [ 1.0489,  0.7899, -1.8661],\n",
            "        [-0.7692, -0.1590, -1.6144]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outp.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  outp = (w + y).mean()\n",
        "\n",
        "print(outp.requires_grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrM4Wuiy4aFj",
        "outputId": "f319ac92-9906-4c5c-bc6f-ad38329240bc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5 - Using Optimizers\n",
        "\n",
        "Copy over the code from the using optimizer notebook, and execute the cells and enter the code with the instructor."
      ],
      "metadata": {
        "id": "TO46rEj8FJmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "CaaOpDDKFJAd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "mean, std = (0.5,), (0.5,)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean, std)\n",
        "                                ])\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download =True, train = True, transform = transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
        "\n",
        "testset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download =True, train = True, transform = transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True)\n",
        "\n",
        "class FMNIST (nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(784, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "  def forward (self, x):\n",
        "\n",
        "    x = x.view(x.shape[0], -1)\n",
        "\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "#model = FMNIST()\n"
      ],
      "metadata": {
        "id": "ieeiprNiJQZY"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128,64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64,10),\n",
        "                      nn.LogSoftmax(dim=1))"
      ],
      "metadata": {
        "id": "RYp_SdbRJQcQ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(trainloader))\n",
        "\n",
        "images = images.view(images.shape[0], -1)"
      ],
      "metadata": {
        "id": "qlplhLC4JQfG"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)"
      ],
      "metadata": {
        "id": "-9qXPc9bJQh6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(images)\n",
        "loss = criterion(output, labels)\n",
        "loss.backward()\n",
        "print('Initial weights : ', model[0].weight)\n",
        "print('Initial weigts gradient : ', model[0].weight.grad)"
      ],
      "metadata": {
        "id": "tG73hJVqJQkm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1a67bcb-ade1-4484-9395-946d9d49aebe"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
            "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0295,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0220, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0286,  0.0350, -0.0105]],\n",
            "       requires_grad=True)\n",
            "Initial weigts gradient :  tensor([[-0.0030, -0.0030, -0.0030,  ..., -0.0030, -0.0030, -0.0030],\n",
            "        [ 0.0022,  0.0022,  0.0022,  ...,  0.0024,  0.0022,  0.0022],\n",
            "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n",
            "        ...,\n",
            "        [ 0.0014,  0.0014,  0.0014,  ...,  0.0014,  0.0014,  0.0014],\n",
            "        [ 0.0021,  0.0021,  0.0021,  ...,  0.0022,  0.0021,  0.0021],\n",
            "        [ 0.0038,  0.0038,  0.0038,  ...,  0.0038,  0.0038,  0.0038]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.step()"
      ],
      "metadata": {
        "id": "S3O5RCcYJQnH"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Initial weights : ', model[0].weight)\n",
        "print('Initial weigts gradient : ', model[0].weight.grad)"
      ],
      "metadata": {
        "id": "NJTif4H7JQpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "866735cd-10d5-4bee-8545-145728fc6271"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0002,  0.0192, -0.0294,  ...,  0.0220,  0.0038,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0105,  ..., -0.0203, -0.0060, -0.0300],\n",
            "        [-0.0202,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0296,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0221, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0065,  0.0125,  ...,  0.0285,  0.0349, -0.0106]],\n",
            "       requires_grad=True)\n",
            "Initial weigts gradient :  tensor([[-0.0030, -0.0030, -0.0030,  ..., -0.0030, -0.0030, -0.0030],\n",
            "        [ 0.0022,  0.0022,  0.0022,  ...,  0.0024,  0.0022,  0.0022],\n",
            "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n",
            "        ...,\n",
            "        [ 0.0014,  0.0014,  0.0014,  ...,  0.0014,  0.0014,  0.0014],\n",
            "        [ 0.0021,  0.0021,  0.0021,  ...,  0.0022,  0.0021,  0.0021],\n",
            "        [ 0.0038,  0.0038,  0.0038,  ...,  0.0038,  0.0038,  0.0038]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "7XtanTi2JuiM"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Initial weights : ', model[0].weight)\n",
        "print('Initial weigts gradient : ', model[0].weight.grad)"
      ],
      "metadata": {
        "id": "RBvrvWmrJulK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "657e09f0-3760-4cfd-85d9-c5743bf84790"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0002,  0.0192, -0.0294,  ...,  0.0220,  0.0038,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0105,  ..., -0.0203, -0.0060, -0.0300],\n",
            "        [-0.0202,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0296,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0221, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0065,  0.0125,  ...,  0.0285,  0.0349, -0.0106]],\n",
            "       requires_grad=True)\n",
            "Initial weigts gradient :  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FMNIST()\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "for i in range(num_epochs):\n",
        "\n",
        "  cum_loss = 0\n",
        "  batch_num = 0\n",
        "\n",
        "  for batch_num, (images, labels)  in enumerate(trainloader, 1):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(images)\n",
        "\n",
        "    loss = criterion(output, labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    cum_loss += loss.item()\n",
        "    print(f'Batch : {batch_num}, Loss : {loss.item()}')\n",
        "\n",
        "  print(f\"Traning loss: {cum_loss/len(trainloader)}\")"
      ],
      "metadata": {
        "id": "d12hptjWJuoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f4d474-09e4-48a5-de97-856379df0c75"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch : 1, Loss : 2.2870397567749023\n",
            "Batch : 2, Loss : 2.304734468460083\n",
            "Batch : 3, Loss : 2.28812313079834\n",
            "Batch : 4, Loss : 2.2898640632629395\n",
            "Batch : 5, Loss : 2.2802863121032715\n",
            "Batch : 6, Loss : 2.2768282890319824\n",
            "Batch : 7, Loss : 2.2971038818359375\n",
            "Batch : 8, Loss : 2.272695779800415\n",
            "Batch : 9, Loss : 2.266519546508789\n",
            "Batch : 10, Loss : 2.276360273361206\n",
            "Batch : 11, Loss : 2.285900115966797\n",
            "Batch : 12, Loss : 2.2687723636627197\n",
            "Batch : 13, Loss : 2.2825448513031006\n",
            "Batch : 14, Loss : 2.269296407699585\n",
            "Batch : 15, Loss : 2.251699447631836\n",
            "Batch : 16, Loss : 2.2710673809051514\n",
            "Batch : 17, Loss : 2.2565464973449707\n",
            "Batch : 18, Loss : 2.247075080871582\n",
            "Batch : 19, Loss : 2.242631673812866\n",
            "Batch : 20, Loss : 2.2493398189544678\n",
            "Batch : 21, Loss : 2.2490806579589844\n",
            "Batch : 22, Loss : 2.239584445953369\n",
            "Batch : 23, Loss : 2.2394418716430664\n",
            "Batch : 24, Loss : 2.252643585205078\n",
            "Batch : 25, Loss : 2.2172629833221436\n",
            "Batch : 26, Loss : 2.236956834793091\n",
            "Batch : 27, Loss : 2.2230703830718994\n",
            "Batch : 28, Loss : 2.223057270050049\n",
            "Batch : 29, Loss : 2.197035074234009\n",
            "Batch : 30, Loss : 2.2178027629852295\n",
            "Batch : 31, Loss : 2.24252986907959\n",
            "Batch : 32, Loss : 2.223618507385254\n",
            "Batch : 33, Loss : 2.182101249694824\n",
            "Batch : 34, Loss : 2.1979775428771973\n",
            "Batch : 35, Loss : 2.219874143600464\n",
            "Batch : 36, Loss : 2.2039496898651123\n",
            "Batch : 37, Loss : 2.219465970993042\n",
            "Batch : 38, Loss : 2.1876096725463867\n",
            "Batch : 39, Loss : 2.1937692165374756\n",
            "Batch : 40, Loss : 2.199791431427002\n",
            "Batch : 41, Loss : 2.1997413635253906\n",
            "Batch : 42, Loss : 2.199578046798706\n",
            "Batch : 43, Loss : 2.166189670562744\n",
            "Batch : 44, Loss : 2.1731696128845215\n",
            "Batch : 45, Loss : 2.1663475036621094\n",
            "Batch : 46, Loss : 2.163022994995117\n",
            "Batch : 47, Loss : 2.173583745956421\n",
            "Batch : 48, Loss : 2.166243076324463\n",
            "Batch : 49, Loss : 2.17478609085083\n",
            "Batch : 50, Loss : 2.1410579681396484\n",
            "Batch : 51, Loss : 2.132265329360962\n",
            "Batch : 52, Loss : 2.1887452602386475\n",
            "Batch : 53, Loss : 2.1620266437530518\n",
            "Batch : 54, Loss : 2.117699146270752\n",
            "Batch : 55, Loss : 2.141117572784424\n",
            "Batch : 56, Loss : 2.135773181915283\n",
            "Batch : 57, Loss : 2.1412599086761475\n",
            "Batch : 58, Loss : 2.1211435794830322\n",
            "Batch : 59, Loss : 2.103712320327759\n",
            "Batch : 60, Loss : 2.105754852294922\n",
            "Batch : 61, Loss : 2.1260485649108887\n",
            "Batch : 62, Loss : 2.0871195793151855\n",
            "Batch : 63, Loss : 2.0897393226623535\n",
            "Batch : 64, Loss : 2.1417415142059326\n",
            "Batch : 65, Loss : 2.0826640129089355\n",
            "Batch : 66, Loss : 2.086456298828125\n",
            "Batch : 67, Loss : 2.0807738304138184\n",
            "Batch : 68, Loss : 2.0981459617614746\n",
            "Batch : 69, Loss : 2.084650993347168\n",
            "Batch : 70, Loss : 2.0585594177246094\n",
            "Batch : 71, Loss : 2.083326816558838\n",
            "Batch : 72, Loss : 2.0672619342803955\n",
            "Batch : 73, Loss : 2.045077085494995\n",
            "Batch : 74, Loss : 2.0248615741729736\n",
            "Batch : 75, Loss : 2.0355846881866455\n",
            "Batch : 76, Loss : 2.0456814765930176\n",
            "Batch : 77, Loss : 2.042203664779663\n",
            "Batch : 78, Loss : 2.0114800930023193\n",
            "Batch : 79, Loss : 2.0007989406585693\n",
            "Batch : 80, Loss : 2.0284688472747803\n",
            "Batch : 81, Loss : 2.050344228744507\n",
            "Batch : 82, Loss : 1.9999030828475952\n",
            "Batch : 83, Loss : 2.040477752685547\n",
            "Batch : 84, Loss : 2.004279613494873\n",
            "Batch : 85, Loss : 2.0411429405212402\n",
            "Batch : 86, Loss : 1.98568594455719\n",
            "Batch : 87, Loss : 1.984741449356079\n",
            "Batch : 88, Loss : 1.9755622148513794\n",
            "Batch : 89, Loss : 1.9781358242034912\n",
            "Batch : 90, Loss : 1.971121907234192\n",
            "Batch : 91, Loss : 2.0088024139404297\n",
            "Batch : 92, Loss : 1.9533545970916748\n",
            "Batch : 93, Loss : 1.9273675680160522\n",
            "Batch : 94, Loss : 1.9663947820663452\n",
            "Batch : 95, Loss : 1.9294841289520264\n",
            "Batch : 96, Loss : 1.8992468118667603\n",
            "Batch : 97, Loss : 1.9139755964279175\n",
            "Batch : 98, Loss : 1.920300841331482\n",
            "Batch : 99, Loss : 1.893148422241211\n",
            "Batch : 100, Loss : 1.9020233154296875\n",
            "Batch : 101, Loss : 1.9861620664596558\n",
            "Batch : 102, Loss : 1.8941391706466675\n",
            "Batch : 103, Loss : 1.8785282373428345\n",
            "Batch : 104, Loss : 1.8526650667190552\n",
            "Batch : 105, Loss : 1.8771803379058838\n",
            "Batch : 106, Loss : 1.8768731355667114\n",
            "Batch : 107, Loss : 1.8324782848358154\n",
            "Batch : 108, Loss : 1.8375012874603271\n",
            "Batch : 109, Loss : 1.8727120161056519\n",
            "Batch : 110, Loss : 1.8134827613830566\n",
            "Batch : 111, Loss : 1.8504546880722046\n",
            "Batch : 112, Loss : 1.8896410465240479\n",
            "Batch : 113, Loss : 1.8046207427978516\n",
            "Batch : 114, Loss : 1.8042620420455933\n",
            "Batch : 115, Loss : 1.7859388589859009\n",
            "Batch : 116, Loss : 1.7643969058990479\n",
            "Batch : 117, Loss : 1.773877501487732\n",
            "Batch : 118, Loss : 1.8144224882125854\n",
            "Batch : 119, Loss : 1.778023600578308\n",
            "Batch : 120, Loss : 1.7875748872756958\n",
            "Batch : 121, Loss : 1.7815160751342773\n",
            "Batch : 122, Loss : 1.7425154447555542\n",
            "Batch : 123, Loss : 1.6732897758483887\n",
            "Batch : 124, Loss : 1.8245290517807007\n",
            "Batch : 125, Loss : 1.7087002992630005\n",
            "Batch : 126, Loss : 1.7019673585891724\n",
            "Batch : 127, Loss : 1.7215553522109985\n",
            "Batch : 128, Loss : 1.688834309577942\n",
            "Batch : 129, Loss : 1.7354899644851685\n",
            "Batch : 130, Loss : 1.6645172834396362\n",
            "Batch : 131, Loss : 1.6880850791931152\n",
            "Batch : 132, Loss : 1.6604924201965332\n",
            "Batch : 133, Loss : 1.6661121845245361\n",
            "Batch : 134, Loss : 1.628056526184082\n",
            "Batch : 135, Loss : 1.6878329515457153\n",
            "Batch : 136, Loss : 1.6056441068649292\n",
            "Batch : 137, Loss : 1.6899101734161377\n",
            "Batch : 138, Loss : 1.5744439363479614\n",
            "Batch : 139, Loss : 1.6058402061462402\n",
            "Batch : 140, Loss : 1.6007808446884155\n",
            "Batch : 141, Loss : 1.5656627416610718\n",
            "Batch : 142, Loss : 1.630824327468872\n",
            "Batch : 143, Loss : 1.6335318088531494\n",
            "Batch : 144, Loss : 1.537968397140503\n",
            "Batch : 145, Loss : 1.589952826499939\n",
            "Batch : 146, Loss : 1.53298819065094\n",
            "Batch : 147, Loss : 1.5864728689193726\n",
            "Batch : 148, Loss : 1.5517463684082031\n",
            "Batch : 149, Loss : 1.6810373067855835\n",
            "Batch : 150, Loss : 1.5942589044570923\n",
            "Batch : 151, Loss : 1.5094536542892456\n",
            "Batch : 152, Loss : 1.5328160524368286\n",
            "Batch : 153, Loss : 1.5457831621170044\n",
            "Batch : 154, Loss : 1.4847909212112427\n",
            "Batch : 155, Loss : 1.5357012748718262\n",
            "Batch : 156, Loss : 1.471389889717102\n",
            "Batch : 157, Loss : 1.547956943511963\n",
            "Batch : 158, Loss : 1.4475404024124146\n",
            "Batch : 159, Loss : 1.4744549989700317\n",
            "Batch : 160, Loss : 1.5116565227508545\n",
            "Batch : 161, Loss : 1.4856828451156616\n",
            "Batch : 162, Loss : 1.391874074935913\n",
            "Batch : 163, Loss : 1.4790599346160889\n",
            "Batch : 164, Loss : 1.4736872911453247\n",
            "Batch : 165, Loss : 1.4298242330551147\n",
            "Batch : 166, Loss : 1.41767418384552\n",
            "Batch : 167, Loss : 1.573803186416626\n",
            "Batch : 168, Loss : 1.4504849910736084\n",
            "Batch : 169, Loss : 1.4643325805664062\n",
            "Batch : 170, Loss : 1.4500443935394287\n",
            "Batch : 171, Loss : 1.3603771924972534\n",
            "Batch : 172, Loss : 1.4941645860671997\n",
            "Batch : 173, Loss : 1.3527286052703857\n",
            "Batch : 174, Loss : 1.4247959852218628\n",
            "Batch : 175, Loss : 1.4404929876327515\n",
            "Batch : 176, Loss : 1.3351829051971436\n",
            "Batch : 177, Loss : 1.4884059429168701\n",
            "Batch : 178, Loss : 1.4485090970993042\n",
            "Batch : 179, Loss : 1.4672387838363647\n",
            "Batch : 180, Loss : 1.4081943035125732\n",
            "Batch : 181, Loss : 1.3293312788009644\n",
            "Batch : 182, Loss : 1.3573660850524902\n",
            "Batch : 183, Loss : 1.3603969812393188\n",
            "Batch : 184, Loss : 1.404849648475647\n",
            "Batch : 185, Loss : 1.3898038864135742\n",
            "Batch : 186, Loss : 1.3744632005691528\n",
            "Batch : 187, Loss : 1.430775761604309\n",
            "Batch : 188, Loss : 1.3570908308029175\n",
            "Batch : 189, Loss : 1.2827564477920532\n",
            "Batch : 190, Loss : 1.2756606340408325\n",
            "Batch : 191, Loss : 1.2754039764404297\n",
            "Batch : 192, Loss : 1.2776994705200195\n",
            "Batch : 193, Loss : 1.211912751197815\n",
            "Batch : 194, Loss : 1.2995885610580444\n",
            "Batch : 195, Loss : 1.3091520071029663\n",
            "Batch : 196, Loss : 1.449568748474121\n",
            "Batch : 197, Loss : 1.2857229709625244\n",
            "Batch : 198, Loss : 1.3289313316345215\n",
            "Batch : 199, Loss : 1.2628917694091797\n",
            "Batch : 200, Loss : 1.241918921470642\n",
            "Batch : 201, Loss : 1.2489590644836426\n",
            "Batch : 202, Loss : 1.3038017749786377\n",
            "Batch : 203, Loss : 1.2043704986572266\n",
            "Batch : 204, Loss : 1.2874938249588013\n",
            "Batch : 205, Loss : 1.2431583404541016\n",
            "Batch : 206, Loss : 1.1754915714263916\n",
            "Batch : 207, Loss : 1.2307859659194946\n",
            "Batch : 208, Loss : 1.140494704246521\n",
            "Batch : 209, Loss : 1.3678163290023804\n",
            "Batch : 210, Loss : 1.261942744255066\n",
            "Batch : 211, Loss : 1.1684428453445435\n",
            "Batch : 212, Loss : 1.319818377494812\n",
            "Batch : 213, Loss : 1.0796616077423096\n",
            "Batch : 214, Loss : 1.117333173751831\n",
            "Batch : 215, Loss : 1.2286982536315918\n",
            "Batch : 216, Loss : 1.1974061727523804\n",
            "Batch : 217, Loss : 1.20839524269104\n",
            "Batch : 218, Loss : 1.2510026693344116\n",
            "Batch : 219, Loss : 1.0543642044067383\n",
            "Batch : 220, Loss : 1.2688978910446167\n",
            "Batch : 221, Loss : 1.27896249294281\n",
            "Batch : 222, Loss : 1.2129831314086914\n",
            "Batch : 223, Loss : 1.1825497150421143\n",
            "Batch : 224, Loss : 1.1791728734970093\n",
            "Batch : 225, Loss : 1.3311454057693481\n",
            "Batch : 226, Loss : 1.1154332160949707\n",
            "Batch : 227, Loss : 1.1277803182601929\n",
            "Batch : 228, Loss : 1.0481840372085571\n",
            "Batch : 229, Loss : 1.1764765977859497\n",
            "Batch : 230, Loss : 1.2211635112762451\n",
            "Batch : 231, Loss : 1.3000301122665405\n",
            "Batch : 232, Loss : 1.150438904762268\n",
            "Batch : 233, Loss : 1.1533279418945312\n",
            "Batch : 234, Loss : 1.1004987955093384\n",
            "Batch : 235, Loss : 1.1316131353378296\n",
            "Batch : 236, Loss : 1.092271327972412\n",
            "Batch : 237, Loss : 1.244462013244629\n",
            "Batch : 238, Loss : 1.098931074142456\n",
            "Batch : 239, Loss : 1.1379446983337402\n",
            "Batch : 240, Loss : 1.1280683279037476\n",
            "Batch : 241, Loss : 1.2034032344818115\n",
            "Batch : 242, Loss : 1.1087865829467773\n",
            "Batch : 243, Loss : 1.1837550401687622\n",
            "Batch : 244, Loss : 1.174514889717102\n",
            "Batch : 245, Loss : 1.126453161239624\n",
            "Batch : 246, Loss : 1.1874589920043945\n",
            "Batch : 247, Loss : 1.141237735748291\n",
            "Batch : 248, Loss : 1.0862828493118286\n",
            "Batch : 249, Loss : 0.9838452935218811\n",
            "Batch : 250, Loss : 1.099269986152649\n",
            "Batch : 251, Loss : 1.0259323120117188\n",
            "Batch : 252, Loss : 1.0281434059143066\n",
            "Batch : 253, Loss : 1.0866336822509766\n",
            "Batch : 254, Loss : 1.0587655305862427\n",
            "Batch : 255, Loss : 1.0446803569793701\n",
            "Batch : 256, Loss : 1.164698600769043\n",
            "Batch : 257, Loss : 1.0904698371887207\n",
            "Batch : 258, Loss : 0.9964821338653564\n",
            "Batch : 259, Loss : 1.0643681287765503\n",
            "Batch : 260, Loss : 1.076379418373108\n",
            "Batch : 261, Loss : 1.0360395908355713\n",
            "Batch : 262, Loss : 1.0996217727661133\n",
            "Batch : 263, Loss : 0.9848845601081848\n",
            "Batch : 264, Loss : 1.0447592735290527\n",
            "Batch : 265, Loss : 1.0897825956344604\n",
            "Batch : 266, Loss : 1.0932960510253906\n",
            "Batch : 267, Loss : 1.1169298887252808\n",
            "Batch : 268, Loss : 1.1510319709777832\n",
            "Batch : 269, Loss : 1.0899593830108643\n",
            "Batch : 270, Loss : 1.1003568172454834\n",
            "Batch : 271, Loss : 1.0718350410461426\n",
            "Batch : 272, Loss : 1.0314911603927612\n",
            "Batch : 273, Loss : 1.0488630533218384\n",
            "Batch : 274, Loss : 1.1296406984329224\n",
            "Batch : 275, Loss : 1.0038394927978516\n",
            "Batch : 276, Loss : 1.1300655603408813\n",
            "Batch : 277, Loss : 1.0546900033950806\n",
            "Batch : 278, Loss : 1.11454439163208\n",
            "Batch : 279, Loss : 1.0150666236877441\n",
            "Batch : 280, Loss : 1.0460901260375977\n",
            "Batch : 281, Loss : 1.1339048147201538\n",
            "Batch : 282, Loss : 0.9761179685592651\n",
            "Batch : 283, Loss : 1.0654934644699097\n",
            "Batch : 284, Loss : 0.9209231734275818\n",
            "Batch : 285, Loss : 1.080431580543518\n",
            "Batch : 286, Loss : 0.9702100157737732\n",
            "Batch : 287, Loss : 0.991675615310669\n",
            "Batch : 288, Loss : 0.9429172277450562\n",
            "Batch : 289, Loss : 1.0510785579681396\n",
            "Batch : 290, Loss : 0.9330555200576782\n",
            "Batch : 291, Loss : 1.0219699144363403\n",
            "Batch : 292, Loss : 1.046406626701355\n",
            "Batch : 293, Loss : 1.0180610418319702\n",
            "Batch : 294, Loss : 1.0389397144317627\n",
            "Batch : 295, Loss : 1.038696527481079\n",
            "Batch : 296, Loss : 1.010002613067627\n",
            "Batch : 297, Loss : 0.9653136134147644\n",
            "Batch : 298, Loss : 0.9908827543258667\n",
            "Batch : 299, Loss : 1.0952587127685547\n",
            "Batch : 300, Loss : 0.9550023674964905\n",
            "Batch : 301, Loss : 0.8943036198616028\n",
            "Batch : 302, Loss : 1.0015144348144531\n",
            "Batch : 303, Loss : 1.0458190441131592\n",
            "Batch : 304, Loss : 0.990409255027771\n",
            "Batch : 305, Loss : 0.9286433458328247\n",
            "Batch : 306, Loss : 0.8938966989517212\n",
            "Batch : 307, Loss : 1.0463982820510864\n",
            "Batch : 308, Loss : 0.9978644251823425\n",
            "Batch : 309, Loss : 0.9947817325592041\n",
            "Batch : 310, Loss : 1.0235381126403809\n",
            "Batch : 311, Loss : 0.8400945663452148\n",
            "Batch : 312, Loss : 0.9030163288116455\n",
            "Batch : 313, Loss : 1.0359846353530884\n",
            "Batch : 314, Loss : 0.8487733602523804\n",
            "Batch : 315, Loss : 1.031097412109375\n",
            "Batch : 316, Loss : 0.8895847797393799\n",
            "Batch : 317, Loss : 0.9951984286308289\n",
            "Batch : 318, Loss : 0.9706331491470337\n",
            "Batch : 319, Loss : 0.8424293994903564\n",
            "Batch : 320, Loss : 1.0455058813095093\n",
            "Batch : 321, Loss : 0.9383759498596191\n",
            "Batch : 322, Loss : 0.9427354335784912\n",
            "Batch : 323, Loss : 1.0756959915161133\n",
            "Batch : 324, Loss : 0.8923154473304749\n",
            "Batch : 325, Loss : 0.8994469046592712\n",
            "Batch : 326, Loss : 0.8154082894325256\n",
            "Batch : 327, Loss : 0.8237285614013672\n",
            "Batch : 328, Loss : 0.9863321185112\n",
            "Batch : 329, Loss : 0.8959536552429199\n",
            "Batch : 330, Loss : 0.8395195603370667\n",
            "Batch : 331, Loss : 0.8635371923446655\n",
            "Batch : 332, Loss : 0.99457186460495\n",
            "Batch : 333, Loss : 0.841984748840332\n",
            "Batch : 334, Loss : 0.989625871181488\n",
            "Batch : 335, Loss : 0.9663033485412598\n",
            "Batch : 336, Loss : 0.9501191973686218\n",
            "Batch : 337, Loss : 0.9832263588905334\n",
            "Batch : 338, Loss : 0.8759115934371948\n",
            "Batch : 339, Loss : 0.8855513334274292\n",
            "Batch : 340, Loss : 0.9511435627937317\n",
            "Batch : 341, Loss : 0.9434106945991516\n",
            "Batch : 342, Loss : 0.7490012645721436\n",
            "Batch : 343, Loss : 0.8747326731681824\n",
            "Batch : 344, Loss : 1.0233350992202759\n",
            "Batch : 345, Loss : 0.8393664360046387\n",
            "Batch : 346, Loss : 0.856907844543457\n",
            "Batch : 347, Loss : 0.8220753073692322\n",
            "Batch : 348, Loss : 0.8316240310668945\n",
            "Batch : 349, Loss : 0.9496402740478516\n",
            "Batch : 350, Loss : 0.9732807874679565\n",
            "Batch : 351, Loss : 0.8508480191230774\n",
            "Batch : 352, Loss : 0.8344924449920654\n",
            "Batch : 353, Loss : 1.0469400882720947\n",
            "Batch : 354, Loss : 0.9938141703605652\n",
            "Batch : 355, Loss : 0.8272982239723206\n",
            "Batch : 356, Loss : 0.8227198719978333\n",
            "Batch : 357, Loss : 0.9246723651885986\n",
            "Batch : 358, Loss : 0.8255587220191956\n",
            "Batch : 359, Loss : 0.9217456579208374\n",
            "Batch : 360, Loss : 1.1145858764648438\n",
            "Batch : 361, Loss : 0.9136335849761963\n",
            "Batch : 362, Loss : 1.0561153888702393\n",
            "Batch : 363, Loss : 0.8876592516899109\n",
            "Batch : 364, Loss : 0.964198648929596\n",
            "Batch : 365, Loss : 1.0136845111846924\n",
            "Batch : 366, Loss : 0.911388635635376\n",
            "Batch : 367, Loss : 0.9671041965484619\n",
            "Batch : 368, Loss : 0.8182908892631531\n",
            "Batch : 369, Loss : 0.6988244652748108\n",
            "Batch : 370, Loss : 0.8112658262252808\n",
            "Batch : 371, Loss : 1.02518892288208\n",
            "Batch : 372, Loss : 0.9694781303405762\n",
            "Batch : 373, Loss : 0.7632172107696533\n",
            "Batch : 374, Loss : 0.9110809564590454\n",
            "Batch : 375, Loss : 0.8206045627593994\n",
            "Batch : 376, Loss : 0.8551994562149048\n",
            "Batch : 377, Loss : 0.7868192791938782\n",
            "Batch : 378, Loss : 1.0272642374038696\n",
            "Batch : 379, Loss : 0.8709172606468201\n",
            "Batch : 380, Loss : 0.974528968334198\n",
            "Batch : 381, Loss : 0.8644379377365112\n",
            "Batch : 382, Loss : 0.7654017806053162\n",
            "Batch : 383, Loss : 0.8527774214744568\n",
            "Batch : 384, Loss : 0.9519807696342468\n",
            "Batch : 385, Loss : 0.8120834231376648\n",
            "Batch : 386, Loss : 0.989862859249115\n",
            "Batch : 387, Loss : 0.9574826955795288\n",
            "Batch : 388, Loss : 0.877681314945221\n",
            "Batch : 389, Loss : 0.7408663630485535\n",
            "Batch : 390, Loss : 0.8576379418373108\n",
            "Batch : 391, Loss : 0.8054624199867249\n",
            "Batch : 392, Loss : 0.8060973286628723\n",
            "Batch : 393, Loss : 0.8291682004928589\n",
            "Batch : 394, Loss : 0.9052843451499939\n",
            "Batch : 395, Loss : 0.8608462810516357\n",
            "Batch : 396, Loss : 0.7162651419639587\n",
            "Batch : 397, Loss : 0.7144876718521118\n",
            "Batch : 398, Loss : 0.777045726776123\n",
            "Batch : 399, Loss : 0.7763360738754272\n",
            "Batch : 400, Loss : 0.7847138047218323\n",
            "Batch : 401, Loss : 0.7018037438392639\n",
            "Batch : 402, Loss : 0.7162075042724609\n",
            "Batch : 403, Loss : 0.8036728501319885\n",
            "Batch : 404, Loss : 0.8699946403503418\n",
            "Batch : 405, Loss : 0.7052884697914124\n",
            "Batch : 406, Loss : 0.8673902750015259\n",
            "Batch : 407, Loss : 0.9441890716552734\n",
            "Batch : 408, Loss : 0.7974622249603271\n",
            "Batch : 409, Loss : 0.759787917137146\n",
            "Batch : 410, Loss : 0.8811392784118652\n",
            "Batch : 411, Loss : 0.7681697010993958\n",
            "Batch : 412, Loss : 0.8312538862228394\n",
            "Batch : 413, Loss : 0.9949191808700562\n",
            "Batch : 414, Loss : 0.7929400205612183\n",
            "Batch : 415, Loss : 0.8652876019477844\n",
            "Batch : 416, Loss : 0.8445001840591431\n",
            "Batch : 417, Loss : 0.7971449494361877\n",
            "Batch : 418, Loss : 0.7811254858970642\n",
            "Batch : 419, Loss : 0.8316574692726135\n",
            "Batch : 420, Loss : 0.8178681135177612\n",
            "Batch : 421, Loss : 0.8016448616981506\n",
            "Batch : 422, Loss : 0.7928270101547241\n",
            "Batch : 423, Loss : 0.8813282251358032\n",
            "Batch : 424, Loss : 0.9164623022079468\n",
            "Batch : 425, Loss : 0.8030794262886047\n",
            "Batch : 426, Loss : 0.8350522518157959\n",
            "Batch : 427, Loss : 0.693718433380127\n",
            "Batch : 428, Loss : 0.6264677047729492\n",
            "Batch : 429, Loss : 0.9206582903862\n",
            "Batch : 430, Loss : 0.8029332160949707\n",
            "Batch : 431, Loss : 0.8649192452430725\n",
            "Batch : 432, Loss : 0.7579404711723328\n",
            "Batch : 433, Loss : 0.8831061124801636\n",
            "Batch : 434, Loss : 0.8506113886833191\n",
            "Batch : 435, Loss : 0.966645359992981\n",
            "Batch : 436, Loss : 0.8275507092475891\n",
            "Batch : 437, Loss : 0.8999677896499634\n",
            "Batch : 438, Loss : 0.8926489353179932\n",
            "Batch : 439, Loss : 0.6166506409645081\n",
            "Batch : 440, Loss : 0.7643369436264038\n",
            "Batch : 441, Loss : 0.8489691019058228\n",
            "Batch : 442, Loss : 0.6933832764625549\n",
            "Batch : 443, Loss : 0.8156134486198425\n",
            "Batch : 444, Loss : 1.0392330884933472\n",
            "Batch : 445, Loss : 0.7406436204910278\n",
            "Batch : 446, Loss : 0.8544393181800842\n",
            "Batch : 447, Loss : 0.7370532751083374\n",
            "Batch : 448, Loss : 0.9395447373390198\n",
            "Batch : 449, Loss : 0.7361971139907837\n",
            "Batch : 450, Loss : 0.7124441862106323\n",
            "Batch : 451, Loss : 0.9102665185928345\n",
            "Batch : 452, Loss : 1.029363989830017\n",
            "Batch : 453, Loss : 0.8122039437294006\n",
            "Batch : 454, Loss : 0.6820266246795654\n",
            "Batch : 455, Loss : 0.7056099772453308\n",
            "Batch : 456, Loss : 0.7315753698348999\n",
            "Batch : 457, Loss : 0.7589622735977173\n",
            "Batch : 458, Loss : 0.5050835013389587\n",
            "Batch : 459, Loss : 0.6925252676010132\n",
            "Batch : 460, Loss : 0.8025827407836914\n",
            "Batch : 461, Loss : 0.8680246472358704\n",
            "Batch : 462, Loss : 0.6756957173347473\n",
            "Batch : 463, Loss : 1.0153707265853882\n",
            "Batch : 464, Loss : 0.9153465628623962\n",
            "Batch : 465, Loss : 0.826843798160553\n",
            "Batch : 466, Loss : 0.9260737895965576\n",
            "Batch : 467, Loss : 0.7050904631614685\n",
            "Batch : 468, Loss : 0.7724119424819946\n",
            "Batch : 469, Loss : 0.7982773780822754\n",
            "Batch : 470, Loss : 0.6500337719917297\n",
            "Batch : 471, Loss : 0.857728898525238\n",
            "Batch : 472, Loss : 0.6799202561378479\n",
            "Batch : 473, Loss : 0.6845473647117615\n",
            "Batch : 474, Loss : 0.8157403469085693\n",
            "Batch : 475, Loss : 0.998626172542572\n",
            "Batch : 476, Loss : 0.709017276763916\n",
            "Batch : 477, Loss : 0.8300089240074158\n",
            "Batch : 478, Loss : 0.7109515070915222\n",
            "Batch : 479, Loss : 0.9293261170387268\n",
            "Batch : 480, Loss : 0.7381426095962524\n",
            "Batch : 481, Loss : 0.8251608610153198\n",
            "Batch : 482, Loss : 0.6785852313041687\n",
            "Batch : 483, Loss : 0.803941011428833\n",
            "Batch : 484, Loss : 0.6891102194786072\n",
            "Batch : 485, Loss : 0.7071841955184937\n",
            "Batch : 486, Loss : 0.726672887802124\n",
            "Batch : 487, Loss : 0.6619581580162048\n",
            "Batch : 488, Loss : 0.6716457605361938\n",
            "Batch : 489, Loss : 0.8216601014137268\n",
            "Batch : 490, Loss : 0.6027671098709106\n",
            "Batch : 491, Loss : 0.7682535648345947\n",
            "Batch : 492, Loss : 0.5616258978843689\n",
            "Batch : 493, Loss : 0.7680515646934509\n",
            "Batch : 494, Loss : 0.7309750914573669\n",
            "Batch : 495, Loss : 0.7196654677391052\n",
            "Batch : 496, Loss : 0.821711003780365\n",
            "Batch : 497, Loss : 0.6385340094566345\n",
            "Batch : 498, Loss : 0.7569483518600464\n",
            "Batch : 499, Loss : 0.912864089012146\n",
            "Batch : 500, Loss : 0.7630908489227295\n",
            "Batch : 501, Loss : 0.6967260837554932\n",
            "Batch : 502, Loss : 0.8025686740875244\n",
            "Batch : 503, Loss : 0.7281137704849243\n",
            "Batch : 504, Loss : 0.6624351143836975\n",
            "Batch : 505, Loss : 0.7102810144424438\n",
            "Batch : 506, Loss : 0.8844426870346069\n",
            "Batch : 507, Loss : 0.6496294736862183\n",
            "Batch : 508, Loss : 0.9447357654571533\n",
            "Batch : 509, Loss : 0.7727077007293701\n",
            "Batch : 510, Loss : 0.7124234437942505\n",
            "Batch : 511, Loss : 0.8909589052200317\n",
            "Batch : 512, Loss : 0.718944787979126\n",
            "Batch : 513, Loss : 0.746975302696228\n",
            "Batch : 514, Loss : 0.8481975793838501\n",
            "Batch : 515, Loss : 0.8862131237983704\n",
            "Batch : 516, Loss : 0.7309679985046387\n",
            "Batch : 517, Loss : 0.7482437491416931\n",
            "Batch : 518, Loss : 0.7538402080535889\n",
            "Batch : 519, Loss : 0.8464041352272034\n",
            "Batch : 520, Loss : 0.804534375667572\n",
            "Batch : 521, Loss : 0.7899157404899597\n",
            "Batch : 522, Loss : 0.6882625818252563\n",
            "Batch : 523, Loss : 0.8227498531341553\n",
            "Batch : 524, Loss : 0.5744245648384094\n",
            "Batch : 525, Loss : 0.676489531993866\n",
            "Batch : 526, Loss : 0.773375928401947\n",
            "Batch : 527, Loss : 0.7027943730354309\n",
            "Batch : 528, Loss : 0.6235061883926392\n",
            "Batch : 529, Loss : 0.6948900818824768\n",
            "Batch : 530, Loss : 0.6463954448699951\n",
            "Batch : 531, Loss : 0.6703481078147888\n",
            "Batch : 532, Loss : 0.8543187379837036\n",
            "Batch : 533, Loss : 0.6025367975234985\n",
            "Batch : 534, Loss : 0.6480562686920166\n",
            "Batch : 535, Loss : 0.8311336040496826\n",
            "Batch : 536, Loss : 0.7631909847259521\n",
            "Batch : 537, Loss : 0.7019103765487671\n",
            "Batch : 538, Loss : 0.6334488391876221\n",
            "Batch : 539, Loss : 0.7253581285476685\n",
            "Batch : 540, Loss : 0.7727627158164978\n",
            "Batch : 541, Loss : 0.7677466869354248\n",
            "Batch : 542, Loss : 0.7240954637527466\n",
            "Batch : 543, Loss : 0.6323587894439697\n",
            "Batch : 544, Loss : 0.642569899559021\n",
            "Batch : 545, Loss : 0.7284311652183533\n",
            "Batch : 546, Loss : 0.6933704018592834\n",
            "Batch : 547, Loss : 0.7115890383720398\n",
            "Batch : 548, Loss : 0.7977623343467712\n",
            "Batch : 549, Loss : 0.8951696753501892\n",
            "Batch : 550, Loss : 0.732413649559021\n",
            "Batch : 551, Loss : 0.7408857941627502\n",
            "Batch : 552, Loss : 0.6816959977149963\n",
            "Batch : 553, Loss : 0.7888883352279663\n",
            "Batch : 554, Loss : 0.7909985184669495\n",
            "Batch : 555, Loss : 0.6947780847549438\n",
            "Batch : 556, Loss : 0.6581935882568359\n",
            "Batch : 557, Loss : 0.6871351599693298\n",
            "Batch : 558, Loss : 0.6917263269424438\n",
            "Batch : 559, Loss : 0.76834636926651\n",
            "Batch : 560, Loss : 0.826384961605072\n",
            "Batch : 561, Loss : 0.8625915050506592\n",
            "Batch : 562, Loss : 0.7536149024963379\n",
            "Batch : 563, Loss : 0.8228625059127808\n",
            "Batch : 564, Loss : 0.8452389240264893\n",
            "Batch : 565, Loss : 0.6722614765167236\n",
            "Batch : 566, Loss : 0.6994682550430298\n",
            "Batch : 567, Loss : 0.5575480461120605\n",
            "Batch : 568, Loss : 0.648880660533905\n",
            "Batch : 569, Loss : 0.6628902554512024\n",
            "Batch : 570, Loss : 0.7153473496437073\n",
            "Batch : 571, Loss : 0.7293372750282288\n",
            "Batch : 572, Loss : 0.6074510812759399\n",
            "Batch : 573, Loss : 0.6286444067955017\n",
            "Batch : 574, Loss : 0.7058134078979492\n",
            "Batch : 575, Loss : 0.8423490524291992\n",
            "Batch : 576, Loss : 0.8548626899719238\n",
            "Batch : 577, Loss : 0.774509608745575\n",
            "Batch : 578, Loss : 0.7538652420043945\n",
            "Batch : 579, Loss : 0.7573028802871704\n",
            "Batch : 580, Loss : 0.6756683588027954\n",
            "Batch : 581, Loss : 0.8390790224075317\n",
            "Batch : 582, Loss : 0.8353062272071838\n",
            "Batch : 583, Loss : 0.7209546566009521\n",
            "Batch : 584, Loss : 0.5637922883033752\n",
            "Batch : 585, Loss : 0.9786086082458496\n",
            "Batch : 586, Loss : 0.7759509086608887\n",
            "Batch : 587, Loss : 0.8211392760276794\n",
            "Batch : 588, Loss : 0.7601385712623596\n",
            "Batch : 589, Loss : 0.49363741278648376\n",
            "Batch : 590, Loss : 0.7239441871643066\n",
            "Batch : 591, Loss : 0.7131495475769043\n",
            "Batch : 592, Loss : 0.5551051497459412\n",
            "Batch : 593, Loss : 0.8185091614723206\n",
            "Batch : 594, Loss : 0.787936806678772\n",
            "Batch : 595, Loss : 0.5757874846458435\n",
            "Batch : 596, Loss : 0.5757899284362793\n",
            "Batch : 597, Loss : 0.7144646048545837\n",
            "Batch : 598, Loss : 0.7753325700759888\n",
            "Batch : 599, Loss : 0.8482393026351929\n",
            "Batch : 600, Loss : 0.5530043244361877\n",
            "Batch : 601, Loss : 0.7412940859794617\n",
            "Batch : 602, Loss : 0.6946490406990051\n",
            "Batch : 603, Loss : 0.6641499400138855\n",
            "Batch : 604, Loss : 0.8878411054611206\n",
            "Batch : 605, Loss : 0.667698323726654\n",
            "Batch : 606, Loss : 0.5106295943260193\n",
            "Batch : 607, Loss : 0.5619401931762695\n",
            "Batch : 608, Loss : 0.8441967964172363\n",
            "Batch : 609, Loss : 0.6429253816604614\n",
            "Batch : 610, Loss : 0.6832432746887207\n",
            "Batch : 611, Loss : 0.6514419913291931\n",
            "Batch : 612, Loss : 0.4856492578983307\n",
            "Batch : 613, Loss : 0.7505178451538086\n",
            "Batch : 614, Loss : 0.6752431988716125\n",
            "Batch : 615, Loss : 0.7284935712814331\n",
            "Batch : 616, Loss : 0.9159020185470581\n",
            "Batch : 617, Loss : 0.588471531867981\n",
            "Batch : 618, Loss : 0.6075939536094666\n",
            "Batch : 619, Loss : 0.6672241687774658\n",
            "Batch : 620, Loss : 0.6240005493164062\n",
            "Batch : 621, Loss : 0.6794690489768982\n",
            "Batch : 622, Loss : 0.6588039398193359\n",
            "Batch : 623, Loss : 0.8498205542564392\n",
            "Batch : 624, Loss : 0.6826983094215393\n",
            "Batch : 625, Loss : 0.7279630303382874\n",
            "Batch : 626, Loss : 0.7015091776847839\n",
            "Batch : 627, Loss : 0.891444742679596\n",
            "Batch : 628, Loss : 0.5709909796714783\n",
            "Batch : 629, Loss : 0.8700380921363831\n",
            "Batch : 630, Loss : 0.8798226118087769\n",
            "Batch : 631, Loss : 0.8560805916786194\n",
            "Batch : 632, Loss : 0.7074726223945618\n",
            "Batch : 633, Loss : 0.7025085687637329\n",
            "Batch : 634, Loss : 0.5990734696388245\n",
            "Batch : 635, Loss : 0.6680814027786255\n",
            "Batch : 636, Loss : 0.610382080078125\n",
            "Batch : 637, Loss : 0.5192089080810547\n",
            "Batch : 638, Loss : 0.5967530608177185\n",
            "Batch : 639, Loss : 0.7870076298713684\n",
            "Batch : 640, Loss : 0.8076353669166565\n",
            "Batch : 641, Loss : 0.6795139908790588\n",
            "Batch : 642, Loss : 0.615434467792511\n",
            "Batch : 643, Loss : 0.6088440418243408\n",
            "Batch : 644, Loss : 0.7840424180030823\n",
            "Batch : 645, Loss : 0.6996870636940002\n",
            "Batch : 646, Loss : 0.866649866104126\n",
            "Batch : 647, Loss : 0.6605151891708374\n",
            "Batch : 648, Loss : 0.6317001581192017\n",
            "Batch : 649, Loss : 0.5797445178031921\n",
            "Batch : 650, Loss : 0.7730209827423096\n",
            "Batch : 651, Loss : 0.6254348158836365\n",
            "Batch : 652, Loss : 0.6406805515289307\n",
            "Batch : 653, Loss : 0.6214316487312317\n",
            "Batch : 654, Loss : 0.8043944835662842\n",
            "Batch : 655, Loss : 0.5760323405265808\n",
            "Batch : 656, Loss : 0.8904306888580322\n",
            "Batch : 657, Loss : 0.6244937777519226\n",
            "Batch : 658, Loss : 0.6512868404388428\n",
            "Batch : 659, Loss : 0.7975203990936279\n",
            "Batch : 660, Loss : 0.8013663291931152\n",
            "Batch : 661, Loss : 0.6938849091529846\n",
            "Batch : 662, Loss : 0.7061808109283447\n",
            "Batch : 663, Loss : 0.8472615480422974\n",
            "Batch : 664, Loss : 0.7706112265586853\n",
            "Batch : 665, Loss : 0.5798915028572083\n",
            "Batch : 666, Loss : 0.6429926156997681\n",
            "Batch : 667, Loss : 0.5610570311546326\n",
            "Batch : 668, Loss : 0.7274981141090393\n",
            "Batch : 669, Loss : 0.825774610042572\n",
            "Batch : 670, Loss : 0.7079207897186279\n",
            "Batch : 671, Loss : 0.7608948945999146\n",
            "Batch : 672, Loss : 0.687532901763916\n",
            "Batch : 673, Loss : 0.8078997135162354\n",
            "Batch : 674, Loss : 0.5496616959571838\n",
            "Batch : 675, Loss : 0.569290816783905\n",
            "Batch : 676, Loss : 0.7319361567497253\n",
            "Batch : 677, Loss : 0.6610420346260071\n",
            "Batch : 678, Loss : 0.6060026288032532\n",
            "Batch : 679, Loss : 0.7571684718132019\n",
            "Batch : 680, Loss : 0.7821866869926453\n",
            "Batch : 681, Loss : 0.807797372341156\n",
            "Batch : 682, Loss : 0.6410341858863831\n",
            "Batch : 683, Loss : 0.6521769165992737\n",
            "Batch : 684, Loss : 0.6756908297538757\n",
            "Batch : 685, Loss : 0.6561317443847656\n",
            "Batch : 686, Loss : 0.6224031448364258\n",
            "Batch : 687, Loss : 0.7800729274749756\n",
            "Batch : 688, Loss : 0.6411485075950623\n",
            "Batch : 689, Loss : 0.6016892194747925\n",
            "Batch : 690, Loss : 0.683422327041626\n",
            "Batch : 691, Loss : 0.6915992498397827\n",
            "Batch : 692, Loss : 0.7474496960639954\n",
            "Batch : 693, Loss : 0.6436126828193665\n",
            "Batch : 694, Loss : 0.7753561735153198\n",
            "Batch : 695, Loss : 0.6228640079498291\n",
            "Batch : 696, Loss : 0.7775707244873047\n",
            "Batch : 697, Loss : 0.8922482132911682\n",
            "Batch : 698, Loss : 0.701455295085907\n",
            "Batch : 699, Loss : 0.6990035176277161\n",
            "Batch : 700, Loss : 0.5417807698249817\n",
            "Batch : 701, Loss : 0.6083142161369324\n",
            "Batch : 702, Loss : 0.561106264591217\n",
            "Batch : 703, Loss : 0.7051206827163696\n",
            "Batch : 704, Loss : 0.7838409543037415\n",
            "Batch : 705, Loss : 0.7032277584075928\n",
            "Batch : 706, Loss : 0.7734262943267822\n",
            "Batch : 707, Loss : 0.799285352230072\n",
            "Batch : 708, Loss : 0.6451100707054138\n",
            "Batch : 709, Loss : 0.5217524766921997\n",
            "Batch : 710, Loss : 0.7310010194778442\n",
            "Batch : 711, Loss : 0.6857199668884277\n",
            "Batch : 712, Loss : 0.5682184100151062\n",
            "Batch : 713, Loss : 0.7596707940101624\n",
            "Batch : 714, Loss : 0.6375705003738403\n",
            "Batch : 715, Loss : 0.5373445153236389\n",
            "Batch : 716, Loss : 0.7729576230049133\n",
            "Batch : 717, Loss : 0.6817507147789001\n",
            "Batch : 718, Loss : 0.5913040041923523\n",
            "Batch : 719, Loss : 0.6283168196678162\n",
            "Batch : 720, Loss : 0.5855664014816284\n",
            "Batch : 721, Loss : 0.5158354640007019\n",
            "Batch : 722, Loss : 0.5784292221069336\n",
            "Batch : 723, Loss : 0.7685644626617432\n",
            "Batch : 724, Loss : 0.5464702248573303\n",
            "Batch : 725, Loss : 0.5220521092414856\n",
            "Batch : 726, Loss : 0.7221256494522095\n",
            "Batch : 727, Loss : 0.5738886594772339\n",
            "Batch : 728, Loss : 0.719257116317749\n",
            "Batch : 729, Loss : 0.5601539015769958\n",
            "Batch : 730, Loss : 0.8209532499313354\n",
            "Batch : 731, Loss : 0.6453717350959778\n",
            "Batch : 732, Loss : 0.6954922080039978\n",
            "Batch : 733, Loss : 0.5901261568069458\n",
            "Batch : 734, Loss : 0.8845281600952148\n",
            "Batch : 735, Loss : 0.5755364298820496\n",
            "Batch : 736, Loss : 0.7365502119064331\n",
            "Batch : 737, Loss : 0.6783444285392761\n",
            "Batch : 738, Loss : 0.5873231291770935\n",
            "Batch : 739, Loss : 0.6442807912826538\n",
            "Batch : 740, Loss : 0.6038832664489746\n",
            "Batch : 741, Loss : 0.724324643611908\n",
            "Batch : 742, Loss : 0.6688640117645264\n",
            "Batch : 743, Loss : 0.5151492953300476\n",
            "Batch : 744, Loss : 0.5529815554618835\n",
            "Batch : 745, Loss : 0.551927924156189\n",
            "Batch : 746, Loss : 0.5993021130561829\n",
            "Batch : 747, Loss : 0.6569493412971497\n",
            "Batch : 748, Loss : 0.7786307334899902\n",
            "Batch : 749, Loss : 0.7211089134216309\n",
            "Batch : 750, Loss : 0.823011040687561\n",
            "Batch : 751, Loss : 0.7234030961990356\n",
            "Batch : 752, Loss : 0.7374319434165955\n",
            "Batch : 753, Loss : 0.7479405403137207\n",
            "Batch : 754, Loss : 0.5696337223052979\n",
            "Batch : 755, Loss : 0.6930347681045532\n",
            "Batch : 756, Loss : 0.5893465876579285\n",
            "Batch : 757, Loss : 0.7851317524909973\n",
            "Batch : 758, Loss : 0.743462860584259\n",
            "Batch : 759, Loss : 0.5102760791778564\n",
            "Batch : 760, Loss : 0.5709173679351807\n",
            "Batch : 761, Loss : 0.616902232170105\n",
            "Batch : 762, Loss : 0.6129447817802429\n",
            "Batch : 763, Loss : 0.791861355304718\n",
            "Batch : 764, Loss : 0.519614040851593\n",
            "Batch : 765, Loss : 0.7498929500579834\n",
            "Batch : 766, Loss : 0.6642195582389832\n",
            "Batch : 767, Loss : 0.49366119503974915\n",
            "Batch : 768, Loss : 0.8068444728851318\n",
            "Batch : 769, Loss : 0.8168132901191711\n",
            "Batch : 770, Loss : 0.8257982134819031\n",
            "Batch : 771, Loss : 0.6554076075553894\n",
            "Batch : 772, Loss : 0.7896862626075745\n",
            "Batch : 773, Loss : 0.6930824518203735\n",
            "Batch : 774, Loss : 0.8821495771408081\n",
            "Batch : 775, Loss : 0.6766883730888367\n",
            "Batch : 776, Loss : 0.7391095161437988\n",
            "Batch : 777, Loss : 0.8051770329475403\n",
            "Batch : 778, Loss : 0.5685008764266968\n",
            "Batch : 779, Loss : 0.7344843745231628\n",
            "Batch : 780, Loss : 0.5793027281761169\n",
            "Batch : 781, Loss : 0.6274312734603882\n",
            "Batch : 782, Loss : 0.573573648929596\n",
            "Batch : 783, Loss : 0.7511330246925354\n",
            "Batch : 784, Loss : 0.5832927823066711\n",
            "Batch : 785, Loss : 0.7028172612190247\n",
            "Batch : 786, Loss : 0.6134808659553528\n",
            "Batch : 787, Loss : 0.7156378030776978\n",
            "Batch : 788, Loss : 0.7578531503677368\n",
            "Batch : 789, Loss : 0.5603702664375305\n",
            "Batch : 790, Loss : 0.5170438289642334\n",
            "Batch : 791, Loss : 0.5373457670211792\n",
            "Batch : 792, Loss : 0.6520823240280151\n",
            "Batch : 793, Loss : 0.6902077198028564\n",
            "Batch : 794, Loss : 0.806830108165741\n",
            "Batch : 795, Loss : 0.6557108759880066\n",
            "Batch : 796, Loss : 0.6068830490112305\n",
            "Batch : 797, Loss : 0.7722467184066772\n",
            "Batch : 798, Loss : 0.6256105303764343\n",
            "Batch : 799, Loss : 0.8847548961639404\n",
            "Batch : 800, Loss : 0.4736250638961792\n",
            "Batch : 801, Loss : 0.5556502342224121\n",
            "Batch : 802, Loss : 0.8184964656829834\n",
            "Batch : 803, Loss : 0.7262576818466187\n",
            "Batch : 804, Loss : 0.7396953701972961\n",
            "Batch : 805, Loss : 0.7877722382545471\n",
            "Batch : 806, Loss : 0.858672022819519\n",
            "Batch : 807, Loss : 0.5271338820457458\n",
            "Batch : 808, Loss : 0.6553995609283447\n",
            "Batch : 809, Loss : 0.6528302431106567\n",
            "Batch : 810, Loss : 0.5448588132858276\n",
            "Batch : 811, Loss : 0.7222935557365417\n",
            "Batch : 812, Loss : 0.679915189743042\n",
            "Batch : 813, Loss : 0.6847376823425293\n",
            "Batch : 814, Loss : 0.6917569041252136\n",
            "Batch : 815, Loss : 0.8162162899971008\n",
            "Batch : 816, Loss : 0.5342095494270325\n",
            "Batch : 817, Loss : 0.8384204506874084\n",
            "Batch : 818, Loss : 0.8520591855049133\n",
            "Batch : 819, Loss : 0.6785795092582703\n",
            "Batch : 820, Loss : 0.6232613921165466\n",
            "Batch : 821, Loss : 0.6667590141296387\n",
            "Batch : 822, Loss : 0.6923292279243469\n",
            "Batch : 823, Loss : 0.6279386281967163\n",
            "Batch : 824, Loss : 0.7132229804992676\n",
            "Batch : 825, Loss : 0.5471465587615967\n",
            "Batch : 826, Loss : 0.8613273501396179\n",
            "Batch : 827, Loss : 0.5713926553726196\n",
            "Batch : 828, Loss : 0.5582960247993469\n",
            "Batch : 829, Loss : 0.5900900959968567\n",
            "Batch : 830, Loss : 0.6410195231437683\n",
            "Batch : 831, Loss : 0.7135322690010071\n",
            "Batch : 832, Loss : 0.7274871468544006\n",
            "Batch : 833, Loss : 0.7111696600914001\n",
            "Batch : 834, Loss : 0.6180930137634277\n",
            "Batch : 835, Loss : 0.605069637298584\n",
            "Batch : 836, Loss : 0.4803294241428375\n",
            "Batch : 837, Loss : 0.507485032081604\n",
            "Batch : 838, Loss : 0.5924387574195862\n",
            "Batch : 839, Loss : 0.6078111529350281\n",
            "Batch : 840, Loss : 0.6823458671569824\n",
            "Batch : 841, Loss : 0.6450344324111938\n",
            "Batch : 842, Loss : 0.7638145685195923\n",
            "Batch : 843, Loss : 0.3884877860546112\n",
            "Batch : 844, Loss : 0.8235077857971191\n",
            "Batch : 845, Loss : 0.7362706661224365\n",
            "Batch : 846, Loss : 0.5621230602264404\n",
            "Batch : 847, Loss : 0.5440495014190674\n",
            "Batch : 848, Loss : 0.5513651371002197\n",
            "Batch : 849, Loss : 0.6541615724563599\n",
            "Batch : 850, Loss : 0.5166135430335999\n",
            "Batch : 851, Loss : 0.428488165140152\n",
            "Batch : 852, Loss : 0.583656907081604\n",
            "Batch : 853, Loss : 0.6126819849014282\n",
            "Batch : 854, Loss : 0.8291776180267334\n",
            "Batch : 855, Loss : 0.7425305843353271\n",
            "Batch : 856, Loss : 0.608963668346405\n",
            "Batch : 857, Loss : 0.5017637014389038\n",
            "Batch : 858, Loss : 0.5401360988616943\n",
            "Batch : 859, Loss : 0.6851924061775208\n",
            "Batch : 860, Loss : 0.5615627765655518\n",
            "Batch : 861, Loss : 0.5974439978599548\n",
            "Batch : 862, Loss : 0.6349262595176697\n",
            "Batch : 863, Loss : 0.6485810875892639\n",
            "Batch : 864, Loss : 0.8013454079627991\n",
            "Batch : 865, Loss : 0.6214755773544312\n",
            "Batch : 866, Loss : 0.5800326466560364\n",
            "Batch : 867, Loss : 0.5958115458488464\n",
            "Batch : 868, Loss : 0.6156319975852966\n",
            "Batch : 869, Loss : 0.6103736758232117\n",
            "Batch : 870, Loss : 0.4533834457397461\n",
            "Batch : 871, Loss : 0.49801698327064514\n",
            "Batch : 872, Loss : 0.5455443859100342\n",
            "Batch : 873, Loss : 0.6277608275413513\n",
            "Batch : 874, Loss : 0.7150675654411316\n",
            "Batch : 875, Loss : 0.7016698718070984\n",
            "Batch : 876, Loss : 0.615506112575531\n",
            "Batch : 877, Loss : 0.6737642884254456\n",
            "Batch : 878, Loss : 0.6567733287811279\n",
            "Batch : 879, Loss : 0.47933685779571533\n",
            "Batch : 880, Loss : 0.6340817213058472\n",
            "Batch : 881, Loss : 0.6810719966888428\n",
            "Batch : 882, Loss : 0.6089551448822021\n",
            "Batch : 883, Loss : 0.7780939340591431\n",
            "Batch : 884, Loss : 0.5369465947151184\n",
            "Batch : 885, Loss : 0.761957049369812\n",
            "Batch : 886, Loss : 0.736850380897522\n",
            "Batch : 887, Loss : 0.6696332693099976\n",
            "Batch : 888, Loss : 0.5798807144165039\n",
            "Batch : 889, Loss : 0.6243416666984558\n",
            "Batch : 890, Loss : 0.5317925214767456\n",
            "Batch : 891, Loss : 0.6085982322692871\n",
            "Batch : 892, Loss : 0.31001266837120056\n",
            "Batch : 893, Loss : 0.6245549321174622\n",
            "Batch : 894, Loss : 0.6164082288742065\n",
            "Batch : 895, Loss : 0.6421104073524475\n",
            "Batch : 896, Loss : 0.491387277841568\n",
            "Batch : 897, Loss : 0.5515141487121582\n",
            "Batch : 898, Loss : 0.6340658664703369\n",
            "Batch : 899, Loss : 0.7303099036216736\n",
            "Batch : 900, Loss : 0.5256715416908264\n",
            "Batch : 901, Loss : 0.5840165615081787\n",
            "Batch : 902, Loss : 0.6183778047561646\n",
            "Batch : 903, Loss : 0.7109900116920471\n",
            "Batch : 904, Loss : 0.7283036112785339\n",
            "Batch : 905, Loss : 0.5285819172859192\n",
            "Batch : 906, Loss : 0.5846195816993713\n",
            "Batch : 907, Loss : 0.6291967630386353\n",
            "Batch : 908, Loss : 0.6800062656402588\n",
            "Batch : 909, Loss : 0.7186949849128723\n",
            "Batch : 910, Loss : 0.6455749273300171\n",
            "Batch : 911, Loss : 0.5345402956008911\n",
            "Batch : 912, Loss : 0.5922220349311829\n",
            "Batch : 913, Loss : 0.6160745024681091\n",
            "Batch : 914, Loss : 0.5296469330787659\n",
            "Batch : 915, Loss : 0.7079765796661377\n",
            "Batch : 916, Loss : 0.5141040682792664\n",
            "Batch : 917, Loss : 0.7219622135162354\n",
            "Batch : 918, Loss : 0.4741613566875458\n",
            "Batch : 919, Loss : 0.47432276606559753\n",
            "Batch : 920, Loss : 0.48167482018470764\n",
            "Batch : 921, Loss : 0.5073999762535095\n",
            "Batch : 922, Loss : 0.4496930241584778\n",
            "Batch : 923, Loss : 0.5409159660339355\n",
            "Batch : 924, Loss : 0.8017846941947937\n",
            "Batch : 925, Loss : 0.5533214807510376\n",
            "Batch : 926, Loss : 0.6625295877456665\n",
            "Batch : 927, Loss : 0.6714104413986206\n",
            "Batch : 928, Loss : 0.5059322118759155\n",
            "Batch : 929, Loss : 0.4800952076911926\n",
            "Batch : 930, Loss : 0.5144814848899841\n",
            "Batch : 931, Loss : 0.5944843888282776\n",
            "Batch : 932, Loss : 0.6778029203414917\n",
            "Batch : 933, Loss : 0.5705801248550415\n",
            "Batch : 934, Loss : 0.6475261449813843\n",
            "Batch : 935, Loss : 0.5637802481651306\n",
            "Batch : 936, Loss : 0.5563649535179138\n",
            "Batch : 937, Loss : 0.5519091486930847\n",
            "Batch : 938, Loss : 0.8960590958595276\n",
            "Traning loss: 1.0100424256024838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5 - Troubleshooting\n",
        "\n",
        "Copy over the code from the using troubleshooting notebook, and execute the cells and enter the code with the instructor."
      ],
      "metadata": {
        "id": "iK1AFRdYLwJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "7FCx_pBJLwXw"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "mean, std = (0.5,), (0.5,)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean, std)\n",
        "                                ])\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download =True, train = True, transform = transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
        "\n",
        "testset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download =True, train = True, transform = transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True)"
      ],
      "metadata": {
        "id": "7WDqnrFQL6iH"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FMNIST (nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(784, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "  def forward (self, x):\n",
        "\n",
        "    x = x.view(x.shape[0], -1)\n",
        "\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "model = FMNIST()"
      ],
      "metadata": {
        "id": "HC8BDnQS851R"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.debugger import set_trace"
      ],
      "metadata": {
        "id": "0oidg5px9OVh"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for i in range(num_epochs):\n",
        "\n",
        "  cum_loss = 0\n",
        "\n",
        "  for images, labels in trainloader:\n",
        "\n",
        "    #set_trace()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(images)\n",
        "\n",
        "    loss = criterion(output, labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    cum_loss += loss.item()\n",
        "\n",
        "  print(f\"Traning loss: {cum_loss/len(trainloader)}\")"
      ],
      "metadata": {
        "id": "dHYq3w4RL6k9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8edd9afe-acc7-4568-b5c4-709ce4285b04"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traning loss: 1.0341349975513752\n",
            "Traning loss: 0.5616475922593684\n",
            "Traning loss: 0.49201103581040145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "images, labels = next(iter(testloader))\n",
        "\n",
        "test_image_id = 0\n",
        "\n",
        "img = images[test_image_id].view(1,784)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  logps = model(img)"
      ],
      "metadata": {
        "id": "vbdzrCkxMx61"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = torch.exp(logps)\n",
        "ps"
      ],
      "metadata": {
        "id": "QIGDyu7FL6p8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b1c8083-1c94-44b1-e029-7dd0b34777e2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.4768e-04, 3.8413e-04, 9.7440e-01, 6.1042e-05, 1.6840e-02, 7.7658e-06,\n",
              "         7.9611e-03, 9.7778e-09, 9.4389e-05, 4.3540e-07]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nps = ps.numpy()[0]\n",
        "nps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zlZ3Yvb-udo",
        "outputId": "2edafdb3-b146-496c-cbd1-8dbe4f1ac6f6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.4768463e-04, 3.8412723e-04, 9.7440356e-01, 6.1041850e-05,\n",
              "       1.6839784e-02, 7.7657978e-06, 7.9611344e-03, 9.7777981e-09,\n",
              "       9.4388779e-05, 4.3539725e-07], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FMNIST_labels = ['T-shirt/top', 'Trouser', 'Pullover', ' Dress', 'Coat', 'Sandal', 'Shirt', 'Sport Shoes', 'Bag', 'Ankle Boot']\n",
        "plt.xticks(np.arange(10), labels = FMNIST_labels, rotation = 'vertical')\n",
        "plt.bar(np.arange(10), nps)"
      ],
      "metadata": {
        "id": "WS5tK_KULwgZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "6004482f-e720-4f2e-9d7e-d8d97707c841"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 10 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def denormalize(tensor):\n",
        "  tensor = tensor * 0.5 + 0.5\n",
        "  return tensor\n",
        "\n",
        "img = img.view(28,-1)\n",
        "img = denormalize(img)\n",
        "plt.imshow(img, cmap = 'gray')"
      ],
      "metadata": {
        "id": "5QjIb8m8MRhZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "8759da67-5471-4d78-9bc9-993e787bc6f6"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0da1bde650>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6 - Validation\n",
        "\n",
        "\n",
        "Copy over the code from the using validation notebook, and execute the cells and enter the code with the instructor."
      ],
      "metadata": {
        "id": "ocIvbj-xIFKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "IEt-MlL9IIyt"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "mean, std = (0.5,), (0.5,)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean, std)\n",
        "                                ])\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download =True, train = True, transform = transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
        "\n",
        "testset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download =True, train = True, transform = transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True)"
      ],
      "metadata": {
        "id": "yq8bVlcyJZJv"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for i in range(num_epochs):\n",
        "\n",
        "  cum_loss = 0\n",
        "\n",
        "  for images, labels in trainloader:\n",
        "\n",
        "    #set_trace()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(images)\n",
        "\n",
        "    loss = criterion(output, labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    cum_loss += loss.item()\n",
        "\n",
        "  print(f\"Traning loss: {cum_loss/len(trainloader)}\")"
      ],
      "metadata": {
        "id": "pbKgFj2gJZMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93cbf8c7-9e1a-4fc0-ffcd-4f55be3df6ed"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traning loss: 0.45657988680578243\n",
            "Traning loss: 0.4329621690327425\n",
            "Traning loss: 0.41544980674918525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "images, labels = next(iter(testloader))\n",
        "\n",
        "test_image_id = 0\n",
        "\n",
        "img = images[test_image_id].view(1,784)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  logps = model(img)"
      ],
      "metadata": {
        "id": "cC9SjhoYJZPd"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = torch.exp(logps)\n",
        "ps"
      ],
      "metadata": {
        "id": "gscDMNKIKX5k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee8dabc-66a2-452d-cab3-39a4e8e17e3d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[9.8903e-01, 1.0600e-04, 1.6774e-03, 3.3503e-03, 2.2979e-06, 9.7775e-07,\n",
              "         5.7244e-03, 2.4246e-08, 1.1011e-04, 1.6907e-08]])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nps = ps.numpy()[0]\n",
        "\n",
        "nps"
      ],
      "metadata": {
        "id": "ww6F-NLoKX8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb48c0d-1868-4bca-ba10-522411d03653"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.8902869e-01, 1.0599661e-04, 1.6773916e-03, 3.3502798e-03,\n",
              "       2.2979441e-06, 9.7775285e-07, 5.7243579e-03, 2.4246347e-08,\n",
              "       1.1010652e-04, 1.6906609e-08], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FMNIST_labels = ['T-shirt/top', 'Trouser', 'Pullover', ' Dress', 'Coat', 'Sandal', 'Shirt', 'Sport Shoes', 'Bag', 'Ankle Boot']\n",
        "plt.xticks(np.arange(10), labels = FMNIST_labels, rotation = 'vertical')\n",
        "plt.bar(np.arange(10), nps)"
      ],
      "metadata": {
        "id": "4G1UUeAvKX-e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "796711eb-3dcf-4326-facd-b9e5f70c3652"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 10 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def denormalize(tensor):\n",
        "  tensor = tensor * 0.5 + 0.5\n",
        "  return tensor\n",
        "\n",
        "img = img.view(28,-1)\n",
        "img = denormalize(img)\n",
        "plt.imshow(img, cmap = 'gray')"
      ],
      "metadata": {
        "id": "wSAr3CQJJZSA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "54337e8e-86e0-4fdd-bd8c-4e1fb09a7662"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0da1ad9750>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "  num_correct = 0\n",
        "  total = 0\n",
        "\n",
        "  cnt = 0\n",
        "\n",
        "  for images, labels in testloader:\n",
        "\n",
        "    logps = model(images)\n",
        "    ourput = torch.exp(logps)\n",
        "    print(output)\n",
        "    cnt += 1\n",
        "\n",
        "    if cnt > 0:\n",
        "\n",
        "      break\n",
        "\n"
      ],
      "metadata": {
        "id": "4AuN8rV3JZU3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e9629d3-7fd2-4e12-fb8a-5e971d2fde91"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.4500e+01, -1.3285e+01, -1.3449e+01, -1.2690e+01, -1.2119e+01,\n",
            "         -5.1302e+00, -1.4789e+01, -7.4008e-03, -8.0650e+00, -6.7843e+00],\n",
            "        [-8.3848e+00, -9.1218e+00, -2.8247e+00, -9.7897e+00, -4.1512e-01,\n",
            "         -1.2033e+01, -1.2746e+00, -1.8952e+01, -7.6688e+00, -1.6317e+01],\n",
            "        [-7.0252e+00, -3.0496e+00, -7.1608e+00, -5.1351e-02, -7.7142e+00,\n",
            "         -1.3002e+01, -9.7792e+00, -7.9744e+00, -8.7610e+00, -1.2281e+01],\n",
            "        [-3.8943e+00, -8.4825e+00, -5.0659e-01, -5.4158e+00, -5.7048e+00,\n",
            "         -8.4456e+00, -1.0063e+00, -1.5422e+01, -5.7018e+00, -1.4099e+01],\n",
            "        [-6.8458e+00, -4.5405e+00, -5.2915e-01, -5.5126e+00, -9.6811e-01,\n",
            "         -1.2314e+01, -4.2101e+00, -1.5522e+01, -7.6475e+00, -1.4874e+01],\n",
            "        [-4.6150e+00, -3.9713e+00, -7.0352e+00, -3.3872e-02, -6.7800e+00,\n",
            "         -1.3161e+01, -6.9600e+00, -6.8567e+00, -7.5507e+00, -1.1669e+01],\n",
            "        [-1.0776e+01, -1.2172e-03, -9.4394e+00, -6.8620e+00, -9.7376e+00,\n",
            "         -1.3614e+01, -1.5127e+01, -1.1697e+01, -1.5664e+01, -1.5532e+01],\n",
            "        [-2.7628e+00, -9.7444e+00, -5.6049e+00, -1.0044e-01, -6.6856e+00,\n",
            "         -1.4416e+01, -3.7188e+00, -1.3572e+01, -5.7452e+00, -1.7190e+01],\n",
            "        [-8.3141e+00, -9.1254e+00, -2.9745e+00, -9.2627e+00, -4.6073e-01,\n",
            "         -1.2234e+01, -1.1485e+00, -1.8007e+01, -7.5270e+00, -1.5692e+01],\n",
            "        [-2.2459e+00, -5.7650e+00, -5.9679e+00, -1.4355e-01, -6.3303e+00,\n",
            "         -1.1600e+01, -4.3470e+00, -8.1238e+00, -4.9396e+00, -1.2070e+01],\n",
            "        [-1.0385e-01, -1.2459e+01, -7.6953e+00, -5.9133e+00, -1.2354e+01,\n",
            "         -2.0046e+01, -2.3491e+00, -2.2424e+01, -1.0565e+01, -2.3154e+01],\n",
            "        [-7.2488e+00, -9.9619e+00, -3.7200e+00, -8.3626e+00, -1.0721e+00,\n",
            "         -1.2608e+01, -4.6169e-01, -1.8743e+01, -6.0947e+00, -1.6232e+01],\n",
            "        [-2.5024e+00, -6.1705e+00, -5.7504e-01, -4.3117e+00, -3.8590e+00,\n",
            "         -8.0067e+00, -1.1674e+00, -1.3620e+01, -4.9180e+00, -1.2643e+01],\n",
            "        [-6.4571e+00, -7.1091e+00, -7.3351e+00, -5.9002e-03, -8.0801e+00,\n",
            "         -1.9671e+01, -5.9832e+00, -1.5177e+01, -1.1257e+01, -1.8561e+01],\n",
            "        [-8.1572e+00, -1.2285e+01, -9.4706e+00, -1.0385e+01, -1.0047e+01,\n",
            "         -2.0004e-01, -8.2280e+00, -2.0731e+00, -3.8725e+00, -3.3819e+00],\n",
            "        [-2.7456e+00, -6.3905e+00, -9.1473e-01, -5.5269e+00, -4.4243e+00,\n",
            "         -5.5342e+00, -7.5515e-01, -1.2246e+01, -3.1360e+00, -8.6395e+00],\n",
            "        [-3.2732e+00, -9.0401e+00, -3.3326e+00, -6.0613e+00, -5.1818e+00,\n",
            "         -5.6845e+00, -2.4454e+00, -9.1735e+00, -1.8927e-01, -7.4409e+00],\n",
            "        [-6.7559e+00, -1.2225e+01, -4.7277e+00, -8.0149e+00, -4.1403e+00,\n",
            "         -1.4023e+01, -2.7500e-02, -2.0953e+01, -7.0584e+00, -1.8682e+01],\n",
            "        [-6.8884e+00, -1.2744e+01, -7.7671e+00, -1.0332e+01, -6.1993e+00,\n",
            "         -6.7909e+00, -6.3546e+00, -7.9375e+00, -6.7864e-03, -1.0283e+01],\n",
            "        [-1.5907e+01, -1.4445e+01, -1.4551e+01, -1.4000e+01, -1.3313e+01,\n",
            "         -4.9348e+00, -1.6472e+01, -8.1440e-03, -8.2953e+00, -7.3147e+00],\n",
            "        [-6.8295e+00, -9.9383e+00, -6.2413e+00, -7.5334e+00, -7.7454e+00,\n",
            "         -2.3271e-01, -6.5923e+00, -2.4419e+00, -2.2053e+00, -5.3011e+00],\n",
            "        [-1.3131e+01, -1.6649e+01, -1.0049e+01, -1.2008e+01, -8.4136e+00,\n",
            "         -8.6353e+00, -1.0519e+01, -1.0284e+01, -5.1473e-04, -1.2954e+01],\n",
            "        [-1.1084e+01, -3.2408e-04, -1.0822e+01, -8.1871e+00, -1.1844e+01,\n",
            "         -1.6258e+01, -1.6594e+01, -1.2676e+01, -1.7907e+01, -1.6696e+01],\n",
            "        [-8.9005e+00, -4.2423e-03, -8.6578e+00, -5.5658e+00, -9.4733e+00,\n",
            "         -1.4811e+01, -1.3465e+01, -1.0966e+01, -1.4892e+01, -1.5125e+01],\n",
            "        [-2.2719e+00, -9.1112e+00, -7.2927e+00, -1.3385e+00, -7.4572e+00,\n",
            "         -1.4637e+01, -4.5754e-01, -1.3028e+01, -7.7203e+00, -1.4666e+01],\n",
            "        [-5.7342e+00, -1.2447e+01, -2.6124e+00, -9.4119e+00, -2.4223e+00,\n",
            "         -9.7117e+00, -2.8705e-01, -1.6624e+01, -2.4761e+00, -1.5961e+01],\n",
            "        [-3.9913e+00, -8.1386e+00, -3.5670e+00, -4.8291e+00, -2.5442e+00,\n",
            "         -6.7359e+00, -1.6339e-01, -1.1710e+01, -4.1351e+00, -1.2413e+01],\n",
            "        [-6.1275e+00, -1.1475e+01, -3.7082e+00, -7.5912e+00, -3.6254e+00,\n",
            "         -1.2854e+01, -5.7079e-02, -2.0464e+01, -6.4253e+00, -1.8381e+01],\n",
            "        [-1.1429e+01, -7.6053e-05, -1.1564e+01, -9.9010e+00, -1.2319e+01,\n",
            "         -1.9723e+01, -1.8936e+01, -1.3683e+01, -2.0378e+01, -1.9989e+01],\n",
            "        [-2.1014e-02, -9.1430e+00, -6.2395e+00, -6.7726e+00, -1.1159e+01,\n",
            "         -1.4793e+01, -4.0469e+00, -1.7094e+01, -9.1917e+00, -1.6036e+01],\n",
            "        [-1.5783e+01, -1.6291e+01, -1.5653e+01, -1.7781e+01, -1.6510e+01,\n",
            "         -2.7702e+00, -1.5325e+01, -4.7287e+00, -1.0054e+01, -7.4216e-02],\n",
            "        [-5.0767e+00, -1.5858e-02, -7.6315e+00, -4.8384e+00, -6.9069e+00,\n",
            "         -1.4181e+01, -1.0038e+01, -1.0177e+01, -1.2331e+01, -1.5532e+01]],\n",
            "       grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  num_correct = 0\n",
        "\n",
        "  tota = 0\n",
        "\n",
        "  for images, labels in testloader:\n",
        "\n",
        "    logps = model(images)\n",
        "    output = torch.exp(logps)\n",
        "\n",
        "    pred = torch.argmax(output, 1)\n",
        "    total += labels.size(0)"
      ],
      "metadata": {
        "id": "ylXG5V2GJZXx"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred, labels"
      ],
      "metadata": {
        "id": "grI7NmXuJZh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76fafc5d-3c6f-4010-d2eb-58aae368e27b"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([3, 3, 4, 9, 7, 3, 3, 2, 6, 4, 9, 2, 5, 7, 9, 6, 4, 3, 4, 4, 7, 6, 9, 5,\n",
              "         7, 5, 9, 7, 4, 1, 0, 2]),\n",
              " tensor([3, 3, 4, 9, 7, 3, 3, 6, 6, 4, 9, 2, 5, 8, 9, 0, 4, 3, 4, 4, 7, 6, 9, 5,\n",
              "         9, 5, 9, 7, 4, 1, 8, 6]))"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred == labels"
      ],
      "metadata": {
        "id": "wR-QznEBJZk9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de20c8e4-1869-4538-ccdf-baa5d3e3f57e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "         True,  True,  True, False,  True, False,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
              "        False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "  num_correct = 0\n",
        "\n",
        "  total = 0\n",
        "\n",
        "  for images, labels in testloader:\n",
        "\n",
        "    logps = model(images)\n",
        "\n",
        "    output = torch.exp(logps)\n",
        "\n",
        "    pred = torch.argmax(output, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "\n",
        "    num_correct += (pred == labels).sum().item()\n",
        "\n",
        "  print(f'Accuracy og the model on the 10000 Test Images: {num_correct * 100 / total}% ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G1p_dGqRJPr",
        "outputId": "f7552e33-4eab-4b2e-86c2-bae9d94711aa"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy og the model on the 10000 Test Images: 85.44833333333334% \n"
          ]
        }
      ]
    }
  ]
}